---
title: "Reinforcement Learning"
author: "Nicholas Lyu"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [refs.bib]
biblio-style: "numeric"
split_bib: yes
link-citations: true
---

\usepackage{cancel}
\usepackage{amsmath, amsfonts}
\usepackage{bm}
\newcommand{\pd}[1]{\partial_{#1}}

\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\mb}{\boldsymbol}
\newcommand{\mrm}{\mathrm}
\newcommand{\mca}{\mathcal}
\newcommand{\mfk}{\mathfrak}
\newcommand{\tr}{\mrm{tr}} 
\newcommand{\df}{\dfrac}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\dag}{\dagger}

\newcommand{\Cl}{\mca C}
\newcommand{\Gr}{\mca G}
\newcommand{\Pf}{\mrm{Pf}}
\newcommand{\Pa}{\mca P}
\newcommand{\R}{\mbb R}
\newcommand{\Exp}{\mbb{E}}
\newcommand{\EV}[1]{\mathop{\mathbb{E}}\limits_{#1}}
\newcommand{\EEV}[2]{%
  \mathop{\mathbb{E}}\limits_{\substack{#1 \\ #2}}}
\newcommand{\argmax}[1]{\operatorname*{arg\,max}_{#1}}

\newcommand{\poly}{\mrm{poly}}



# Preface {-}
These notes accompany the summer 2025 self-learning for _Reinforcement Learning_. Reference resources include: 

- Harvard CS1840's textbook [an Introduction to Reinforcement Learning](https://rlbook.adzc.ai/) by Alexander Cai. 
- [Stanford CS234](https://web.stanford.edu/class/cs234/) lecture notes and [lectures](https://www.youtube.com/watch?v=WsvFL-LjA6U&list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX) by professor Emma Brunskill. 

## Chapter highlights {-}

1. [Markov Decision Processes](#mdp) explores closed-form solutions to finite and infinite-horizon MDPs with **known dynamics and rewards**.
    a. Recursive and optimality operators; contraction properties.
    b. Control problem can be reduced to iterate (policy evaluation + greedy action), i.e. policy iteration. 
2. [Sample-based MDP solutions](#sampMDP) explores MDP solutions with **unknown dynamics, known rewards**. 
    - Estimate procedures: 
        - Monte-Carlo: high-variance no bias, no Markov assumptions, converges to MSE-optimal estimates. 
        - Temporal difference: low variance high bias, need Markov conditions; converges to DP solution on empirical distribution 
    - Monte-Carlo policy iteration: MC + PI, online on-policy. 
    - SARSA: TD policy iteration, online on-policy 
    - Q-learning: action-value iteration; can use TD (canonical) or MC to approximate action-value function, then act greedily. 
3. [Policy Methods](#polMethods) explores an orthogonal approach by _directly optimizing policies_. Foundational policy gradient theorem \@ref(thm:PG) is improved in two ways: 
    - Reducing variance: baseline theorem \@ref(thm:pgBaseline). 
        - The best baseline is state-value function; so good policy methods are contingent upon value method models. 
    - Using off-policy data: relative policy difference theorem \@ref(thm:PDL). 
        - **Surrogate policy loss** corrects for off-policy data contingent upon policy proximity \@ref(def:surrogatePG). Natural extension to PPO-Clip algorithm \@ref(def:ppoClip). 
        - **V-trace** corrects the observed values by (clipped) importance sampling; this corrects both value and policy losses. 
4. [Inverse RL](#iRL) explores MDP solutions with **unknown rewards**, replacing them with estimates from expert demonstration; orthogonal to dynamics estimation methods. 
    - Main problem: identifiably problem of deducing rewards from expert policies; a partial answer is the reward shaping theorem \@ref(thm:rewardShapingTheorem). 
    - Foundational result is [max-entropy IRL](#maxEntropyiRL) which has clean closed-form optimization. 
5. [RL for LLMs](#rlLLMs): eliciting rewards from preferences; direct preference optimization. 
6. [Exploration and bandits](#exploration): regret framework for analyzing decision optimality. Single-state memoryless MDP serves as the foundational block for MDP exploration methods. 
    - Lai-Robbins lower bound \@ref(thm:regretLowerBound): fundamental lower bound on how much trial and error are needed. 
    - Upper-confidence bound bandits dominate lower-confidence bound ones. **Be an optimist in life**!
    - UCB regret bound \@ref(thm:ucbRegret). 
    - Given priors (in a Bayesian setting), use Thompson sampling. 

  
Salient themes: 

1. Methodological approach: exact solutions in strong models (finite MDPs with known dynamics). RL consists of a diverse toolkit for tackling relaxed assumptions. 
    - _Large state-action space_: use deep model function approximation. 
    - _Unknown dynamics_: use sampling; interpolate between temporal-difference and monte-carlo. 
    - _Unknown rewards_: reward modeling (e.g. using max-entropy). 
    - _Exploration_: entropy bonus, bandit-inspired optimism. 
2. Two foundational cornerstones in RL: **(action-)value function** and **policy**. The first is contingent upon DP reductions based on Markov assumptions, giving models more "bite" when assumptions hold. 
    - Accurate action-value function implies good policy: Q-learning. 
    - Policy gradient training can benefit from good value-function estimates. 
3. Applicable themes in life: 
    - Policy-gradient theorem: weighing individual actions by rollout rewards is unbiased despite seeming greedy. 
      - Improve by bootstrapping or baseline. 
      - Correct for off-policy by importance sampling. 
    - Be an optimist under uncertainty! Better to be battered by and learn from reality than missing out. 


## Cool theorems {-} 

- [Markov Decision Processes](#mdp): contraction properties \@ref(thm:contraction), optimality of $Q^*$-greedy policies \@ref(thm:greedyPolicyOptimal). Convergence bound of VI and PI. 
- [Sample-based MDP solutions](#sampMDP): NA, just definition. 
- [Policy methods](#polMethods): PG theorem \@ref(thm:PG), baseline theorem \@ref(thm:pgBaseline), relative policy difference theorem \@ref(def:surrogatePG). 
  - Proof of relative policy performance theorem is a canonical example applying large-deviation theory and $f$-divergences. 
- [Inverse RL](#iRL): reward shaping theorem \@ref(thm:rewardShapingTheorem), max-entropy reduction theorem \@ref(thm:maxEntReduction). 
  - Proof theme: Boltzmann methods, and interchange between single and nested optimizations. 
- [RL for LLMs](#rlLLMs): [DPO reduction](#dpo); variational characterization of KL and using analytic solutions to inner optimizations. 
- [Bandits and exploration](#exploration): Lai-Robbins lower bound \@ref(thm:regretLowerBound) and UCB regret bound \@ref(thm:ucbRegret). 
  - Proof theme: split event into "typical" and "atypical" subevents according to likelihood ratio; typical subevents provide direct bound, and bound atypical subevents using Radon-Nikodym derivative & KL properties. 