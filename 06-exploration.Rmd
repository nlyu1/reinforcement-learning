# Exploration

1. Multi-armed bandit (MAB) = MDP with memoryless environment and one state. It provides a reduced case for analyzing exploration v. exploitation. 
    - Generalization to $|\mca S|>1$ states = **contextual bandit** = MDP with memoryless environment. 
2. Lai-Robbins lower bound (theorem \@ref(thm:regretLowerBound)): regret grows at least as $\log t$. <span style="color:blue">This is an exemplary proof using deviations theory.</span>
3. Optimism dominates pessimism in face of uncertainty: 
    - UCB algorithm and regret bound \@(thm:ucbRegret). 
    - LCB algorithm is inherently greedy. 
4. [Bayesian setup](#bayesianBandit): bayesian regret and Thompson sampling, which implements probability matching.  

## Multi-arm bandit {#bandits -}

:::{.definition #multiArmBandit name="multiarmed bandits"}
A multi-armed bandit consists of a known set of $m$ actions, $\mca R^a(r)=\mbb P(r\mid a)$ an unknown yet stationary distribution over rewards. At each step, the agent selects action $a_t\in \mca A$ and the environment generates a reward. The goal is the maximize the cumulative reward $\sum_{\tau=1}^t r_\tau$. 
:::

:::{.definition name="regret"}
Consider the following definitions; note that recall is a statistical average definition, and that minimizing regret = maximizing expected cumulative reward. 

- **Action-value** $Q(a)=\mbb E[r\mid a]$ denotes the mean reward for action $a$. 
- **Optimal value** $V^* = Q(a^*) = \max_a Q(a)$
- **Regret** is the opportunity loss for one step $I_t = \mbb E[V^* - Q(a_t)]$. 
- **Cumulative regret** $L_t = \sum_{\tau \leq t} I_\tau$. 

Some auxiliary definitions: 

- Average payoff $\mu_a = \mbb E_{\mca R^a}(r)$. 
- **Gap** $\Delta_a = \mu_{a^*} - \mu_a>0$ for each sub-optimal arm. 
- **Pull counts** $N_a(t)$. Under this definition, we have cumulative regret 
\[ 
    L_t = \sum_a \Delta_a \cdot \mbb E\left[N_a(t)\right]
\] 
:::

## Lai-Robins lower bound {#lrLB -} 

:::{.remark #lrLB name="proof strategy"}
Proof for the following theorem is adapted from [appliedprobability.blog](https://appliedprobability.blog/2020/11/25/lai-robbins-lower-bound/). It is an exemplary proof from large deviations theory: 

1. Fixing a suboptimal arm $a$ and bound sequence $n_t$, we wish to prove $P(N_a(t) \leq n_t) = o(1)$ 
2. Construct an alternate measure $P'$ under which $E_t=\{N_a(t) \leq n_t\}$ is $P$-likely yet $P'$-unlikely. 
3. <span style="color:blue">**Key reduction**: let $C_t$ denote the event that <u>empirical KL is $\epsilon$-close to true KL</u>, then 
\[ 
    P(E_t) = P(E_t \cap \overline{C_t}) + P(E_t \cap C_t)
\] 
    - The first term is $o(1)$ by WLLN. 
    - The second term can be converted to a $P'(E_t\cap C_t)$ by Radon-Nikodym derivative, which is bounded by $C_t$ assumption. 
</span>
:::

:::{.theorem #regretLowerBound name="Lai-Robbins lower bound"}
For any algorithm such that the number of suboptimal pulls is $o(t^{\eta\in (0, 1)})$, we have a logarithmic lower bound on regret: 
\begin{align}
    L_t \geq (1-\eta) \log t \sum_{a\neq a^*} \df{\Delta_a}{D(\mca R^a \| \mca R^{a^*})}
\end{align}
:::

_Proof:_ Fixing any suboptimal arm $a$, construct a new bandit instance in which $a$ is optimal, i.e. $\tilde {\mca R}^a$ satisfies $\mbb E[\tilde {\mca R}^a] > \mu_{a^*}$. Let the original measure be denoted $P$ and the new measure $P'$. Fix a bound sequence $n_t$; elementary probability yields 
\begin{align}
    \mbb E_P[N_a(t)] 
    &\geq n_t \left(1 - P[N_a(t) \leq n_t]\right)
\end{align}
We seek to choose $n_t$ such that $P[N_a(t) \geq n_t]=o(1)$. Upper-bound this term by splitting it into two cases: 

- The first case denotes the probability that the deviation between $P$ and $P'$ is not close to that given by KL. goes to $\delta_t = o(1)$ by weak law of large numbers. 
- The second case isolates a case where the deviation between $P, P'$ is $\epsilon$-close to that given by KL. 

Note that conditioning on actions, the bandit environment tensorizes; let $D(r_k)=\log \df{P'(r_k)}{P(r_k)}$ denote the random variable such that $\mbb E[e^D] = dP'/dP$ and $\mbb E[D] = D(P'\|P)$, then 
\begin{align}
    P[N_a(t) \leq n_t] 
    &= P\left[
        N_a(t) \leq n_t \text{   and   } 
        \left| 
            D(P\|P') - \df 1 {n_t} \sum_{k=1}^{n_t} D(r_k)
        \right| > \epsilon 
    \right] \\ 
    &\quad + P\left[
        N_a(t) \leq n_t \text{   and   } 
        \left| 
            D(P\|P') - \df 1 {n_t} \sum_{k=1}^{n_t} D(r_k)
        \right| \leq \epsilon 
    \right] \\ 
    &\leq P\left[
       \left| 
            D(P\|P') - \df 1 {n_t} \sum_{k=1}^{n_t} D(r_k)
        \right| > \epsilon 
    \right] \\ 
    &\quad + \mbb E_{P'}\left[
        \exp \left(
            \sum_{k=1}^{n_t} D(r_k) 
        \right)_{\leq n_t [D(P\|P')+\epsilon]} P' \left[
        N_a(t) \leq n_t
        \right]
    \right] \\ 
    &\leq \delta_t + \exp \left( 
        n_t [D(P\|P') + \epsilon]
    \right) P'[N_a(t) \leq n_t] 
\end{align}

It remains to bound $P'[N_a(t) \leq n_t]$ by Markov's inequality and choose $n_t$: w.l.o.g suppose that the algorithm pulls the suboptimal arm at rate $o(t^\eta)$ for $\eta\in (0, 1)$, we have 
\[ 
    P'[N_a(t) \leq n_t] = P'(t - N_a(t) \geq t - n_T) 
    \leq \df{\mbb E_{P'}[t - N_a(t)]}{t-n_t} = o(t^{\eta - 1}) 
\] 
Substituting, we obtain the bound on 
\begin{align}
    P'[N_a(t) \leq n_t] 
    &\leq \delta_t + \exp \left( 
        n_t [D(P\|P') + \epsilon]
    \right) o(t^{\eta - 1})
\end{align}
The largest choice of $n_t$ which keeps the whole bound $o(1)$ is 
\[ 
    n_t = \df{1-\eta}{D(P\|P') + \epsilon} \log t, \quad 
    \mbb E_P[N_a(t)] \geq n_t (1 - o(1)) 
\] 
Take $\epsilon\to 0$; also note that by chain rule, $D(P\|P') = \mbb E_P[N_a(t)] D(\mca R^a \|\tilde {\mca R}^a) \geq D(\mca R^a \|\tilde {\mca R}^a)$. Take $\tilde {\mca R^a}\to \mca R^{a^*}$ and substituting into the regret formula yields the desired bound. 

## Upper confidence bound method {#ucb -}

The bandit setup highlights the tradeoff between exploration and exploitation. Note that $\epsilon$-greedy methods for fixed $\epsilon$ can invoke linear regret: 

- If $\epsilon>0$, regret $\propto \epsilon/m \min_{a\neq a^*} \Delta_a$
- If $\epsilon=0$, we might be pulling a suboptimal arm greedily forever. 
- In general, the decay schedule of $\epsilon$ is problem-dependent to achieve sublinear regret. 

:::{.theorem #hoeffding name="Hoeffding's inequality"}
Given i.i.d. random variables $(X_j)\in [0, 1]$ and let $\bar X_n$ be the sample mean, then 
\[ 
    P\left[\mbb E[X] > \bar X_n + u\right] \leq \exp(-2nu^2)
\] 
Rearrange and use symmetry to obtain a bound on the absolute value difference 
\[ 
    P\left[\left|\mbb E[X] - \bar X_n\right| > \sqrt{\df{\log(2/\delta)}{2n}}\right] \leq \delta 
\] 
:::

:::{.definition #ucb name="UCB algorithm"}
Given a MAB (multi-arm bandit) with $m$ arms: 

1. Sample once from each arm. 
2. Let $N_a$ denote the number of times arm $a$ has been pulled, and $\hat Q(a)$ be the sample mean reward of arm $a$, choose 
\[ 
    a_t = \argmax{a} \hat Q(a) + \sqrt{\df{\log(2/\delta_t)}{2n_a}} \equiv \argmax a \mrm{UCB}(a, t), \quad \delta_t = 1/t^2 
\] 
:::

:::{.theorem #ucbRegret name="UCB regret bound"}
Using the UCB algorithm as in \@ref(def:ucb), the regret is 
\[ 
    L_t = \sum_{\Delta_a \neq 0} \Delta_a \mbb E[n_a(t)] \leq \Omega\left(
        \log t \sum_{\Delta_a\neq 0} \df 1 {\Delta_a}
    \right)
\] 
:::

<details>
<summary>Assuming that bounds are satisfied, analyze relation between $\delta_t, n_a, Q(a), \hat Q(a), Q(a^*)$</summary>
For a suboptimal arm $a$ to be pulled at time $t$, we have $\mrm{UCB}(a, t) \geq \mrm{UCB}(a^*, t)$. We next break the scenario into two cases:

1. **High-probability case**: The bound for $a$ hold on both sides, and $\mrm{UCB}(a^*, t) \geq Q(a^*)$. Then 
\[ 
    \hat Q(a) - \sqrt{\df{\log(2/\delta_t)}{2n_a}} \leq Q(a) \leq \hat Q(a) + \sqrt{\df{\log(2/\delta_t)}{2n_a }}
\] 
Additionally, the upper bound must exceed that of the optimal arm;  optimal arm holds as well, we have 
\begin{align}
    Q(a^*) \leq \mrm{UCB}(a^*, t) \leq \hat Q(a) + \sqrt{\df{\log(2/\delta_t)}{2n_a }}
    \leq Q(a) + 2 \sqrt{\df{\log(2/\delta_t)}{2n_a }}
\end{align}
Rearrange to yield 
\[ 
    Q(a^*) - Q(a) = \Delta_a \leq 2 \sqrt{\df{\log(2/\delta_t)}{2n_a }} \implies n_a \leq \df{2}{\Delta_a^2} \log \df{2}{\delta_t} 
\] 
2. **Low-probability case**: bound for $a$ doesn't hold when $a$ is chosen at time $t$, or if $\mrm{UCB}(a^*, t) \leq Q(a^*)$; in this case there're no guarantees on the number of times $a$ could have been pulled so the worst case is $t$. 
    - The probability that the bound for $a$ doesn't hold when $a$ is chosen at time $t$ is just $\delta_t$. 
    - For the second component, $P(\mrm{UCB}(a^*, t) \leq Q(a^*))$ is upper bounded by the probability that the bound for $a^*$ doesn't hold for any of time steps $1, \dots, t$, which is $\sum_{\tau=1}^t \delta_\tau$. 

Combining the bounds and choose $\delta_t = 1/t^2$ so that $\sum_{\tau=1}^t \delta_\tau = \Omega(1/t)$, we obtain 
\begin{aligned}
    \mbb E[n_{a\neq a^*}]
    &\leq \mbb E\left[n_{a\neq a^*} \mid \text{high-prob case}\right] + n P(\text{low-prob case})  \\ 
    &\leq \df{2}{\Delta_a^2} \log \df{2}{\delta_t} + n\left(
        \delta_t + \sum_{\tau=1}^t \delta_\tau 
    \right) \\ 
    &\leq \df{2}{\Delta_a^2} \log \df{2}{\delta_t} + t^{-1} + t\,  \Omega(t^{-1}) = \Omega \left(\df{\log t}{\Delta_a^2}\right)
\end{aligned}
</details>

:::{.remark name="LCB algorithm"}
Note that an algorithm which selects the action with greatest lower confidence bound is inherently greedy. When $Q(a^*)>Q(a)>\mrm{UCB}(a)>\mrm{UCB}(a^*)$, the algorithm never explores $a^*$. 
:::

## Bayesian bandit {#bayesianBandit -} 

While frequentist inference assumes that there is a fixed distribution parameter, Bayesian inference assumes that there is a prior over parameters. Regret is computed similarly as 

\[ 
    L^{\mrm{Bayes}}_t = \EEV{\theta\sim p_\theta}{a_t\sim \tau} \left[
        \sum_{\tau=1}^t Q(a^*) - Q(a_t) 
        \bigg| \theta
    \right]
\] 

:::{.definition #probMatching name="probability matching"}
In a Bayes setting, a probability matching algorithm selects action $a$ according to the probability that $a$ is the optimal action, given the current history 
\[ 
    \pi(a\mid h_t) = P\left[
        Q(a) > Q(a'\neq a) \mid h_t 
    \right]
\] 

1. Note that $Q(a), Q(a')$ here are inherently scalars. 
2. The probability is taken over distribution of reward distribution parameters $\{\theta_a\}$, conditioned upon history $h_t$. 
:::

:::{.definition #thompsonSamp name="Thompson sampling"}
Given priors $p(\mca R_a)$, for each iteration repeat:

1. For each arm, sample a reward **distribution parameter** from the parameter posterior $\hat \theta_a \sim \left(p(\mca R_a)\mid h_t\right)$. 
2. Compute the sampled distribution's mean $Q(a) = \mbb E_{\theta_a} \mca R_a(\theta_a)$. 
3. Select $a = \argmax a Q(a)$ and observe reward. 
4. Update posterior for arm $a$. 
:::

Note that probability matching is inherently optimistic in case of uncertainty. Also note that we sample a reward distribution parameter and analytically compute the mean reward, given that parameter. 

## Exploration in MDPs {#mdpExploration -}

:::{.definition name="PAC control"}
Given a MDP, a control algorithm is probably approximately correct (PAC) if on each step, the chosen action $a$: 

- is $\epsilon$-optimal: $Q(s, a)\geq Q(s, a^*) - \epsilon$. 
- with probability at least $1-\delta$ 

on all but $\mrm{poly}(\mca |A|, \epsilon, \delta, \dots)$ number of steps. 
:::