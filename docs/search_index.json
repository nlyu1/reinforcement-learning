[["index.html", "CS 184, Summer 2025 Preface", " CS 184, Summer 2025 Nicholas Lyu 2025-06-27 Preface These notes accompany the summer 2025 self-learning for Reinforcement Learning. "],["mdp.html", "1 Markov Decision Processes Preliminaries Value functions Optimality Policy evaluation Optimal solutions", " 1 Markov Decision Processes This section explores solutions to finite and infinite-horizon MDPs with finite state and action spaces, assuming their dynamics and rewards are totally known. MDP definition 1.1 and trajectory measure \\(\\rho^\\pi\\) 1.2. Definition of value \\(V(s)\\) and action-value \\(Q(s, a)\\) functions 1.3; their inter-relation yields recursive definitions 1.1, which are succinctly encoded in the Bellman consistency operators 1.4. Note that \\(\\mathcal J^\\pi_v\\) samples \\(a\\), then \\(s&#39;\\) dependent upon \\(a\\). On the other hand, \\(\\tilde {\\mathcal J}^\\pi_q\\) samples \\(s&#39;\\), then \\(a&#39;\\) dependent upon \\(s&#39;\\). This difference allows \\(Q\\)-function relations (\\(s&#39;\\) first) to be conveniently sampled from the environment, justifying its prominence in fitted-DP methods. In a sense, \\(Q(s, a)\\) natively encodes “one more layer” of the environment dynamics. The policy-evaluation problem \\(\\pi \\mapsto V^\\pi\\) is solved by backward induction (finite horizon) or iterated convergence (infinite horizon); the latter case results from the contraction properties of consistency operators (theorem 1.3). Optimality relations for \\(V^*, Q^*\\) (theorem 1.1), which are encoded in optimality operators 1.6; optimality operators are also shown to be contractions 1.3. Greedy policy (definition 1.7). It is also shown that \\(V^{\\pi^{V^*}} = V^*\\): acting greedily w.r.t. \\(V^*\\) accumulates value \\(V^*\\). Note that \\(\\pi^q\\) can be computed without knowledge of environment dynamics. Finite-horizon MDPs are solved by backward induction using the optimality relations (definition 1.9). Infinite-horizon MDP control problems are solved by iterative approaches: Value iteration (proposition 1.2) iterate \\(v_j\\to V^*\\) using the optimality operator, then act greedily w.r.t. \\(v_T\\). Acting greedily w.r.t. a candidate value evaluation (which might not be a consistent value function) does not guarantee the payoff: \\(\\gamma\\)-dependent bound 1.4. Policy iteration (theorem 1.5): iterate \\(\\pi \\mapsto \\pi^{V^\\pi}\\), i.e. compute value of current policy, then update policy with one more step of foresight by making locally optimal choices according to the computed value. Intuitively, each iteration gives “one more step of foresight” to the associated quantities. VI and PI highlight two fundamentally different methods to solving the control problem: VI: recurse the optimality operator, then propose a greedy policy. PI: evaluate the current policy, then act greedily w.r.t. the evaluation (fundamentally online). Preliminaries Definition 1.1 (finite-horizon Markov decision processes (MDP)) A finite MDP component consists of: A time horizon \\(H\\in \\mathbb N\\cup \\{+\\infty\\}\\) which specifies the number of interactions. State space \\(\\mathcal S\\). A single state is denoted \\(s\\). Action space \\(\\mathcal A\\). An initial distribution \\(\\mu \\in \\Delta(\\mathcal S)\\). State transition \\(P:\\mathcal S\\times \\mathcal A\\to \\Delta(\\mathcal S)\\). A reward function \\(r:\\mathcal S\\times \\mathcal A\\to \\mathbb R\\). We consider deterministic reward functions. Definition 1.2 (policy, trajectory) A policy is a mapping from state to action: \\[ \\pi:\\mathcal S\\to \\Delta(\\mathcal A) \\] A trajectory is a tuple of interactions: \\[ \\tau = (s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1}) \\] A policy \\(\\pi\\) induces a measure \\(\\rho^\\pi\\) on trajectories given by \\[\\begin{align} \\log \\rho^\\pi(\\tau) &amp;= \\log \\left[\\mathbb P(s_0, a_0, r_0) \\prod_{t=1}^{H-1} \\mathbb P(s_t, a_t, r_t | s_{t-1}, a_{t-1})\\right] \\\\ &amp;= \\log \\mu(s_0) + \\log \\pi(a_0|s_0) + \\sum_{t=1}^{H-1} \\log P(s_t|s_{t-1}, a_{t-1}) + \\log \\pi(a_t|s_{t-1}) \\end{align}\\] To accomodate \\(H=\\infty\\), we can also introduce a discount factor \\(\\gamma\\in [0, 1]\\). The core goal of an RL algorithm is to find the policy \\[ \\pi^* = \\operatorname*{arg\\,max}_{\\pi }\\mathop{\\mathbb{E}}\\limits_{\\rho^\\pi}[G(\\tau)], \\quad G(\\tau) = \\sum_{t=0}^H \\gamma^t r_t \\] Note that the total reward \\(R\\) is a \\(\\tau\\)-dependent random variable. Value functions Given a policy, we need to evaluate its utility. Definition 1.3 ((action-) value function) Given policy \\(\\pi\\), its value function \\(V^\\pi:\\mathcal S\\to \\mathbb R\\) is defined by \\[ V^\\pi_h(s) = \\mathop{\\mathbb{E}}\\limits_{\\tau\\sim \\rho^\\pi}[R_h(\\tau)\\mid s_h=s] \\] here \\(R_h=r_h+\\dots+\\gamma^{\\dots} r_{H-1}\\) is the truncated reward. Similarly, the action-value function is \\[ Q^\\pi_h(s, a) = \\mathop{\\mathbb{E}}\\limits_{\\tau\\sim \\rho^\\pi}[R_h(\\tau)\\mid s_h=s, a_h=a] \\] Note that the finite horizon makes \\(V^\\pi_h\\neq V^\\pi_{h-1}\\); the \\(h\\)-dependence can be dropped when \\(h=\\infty\\). Proposition 1.1 \\(V\\) and \\(Q\\) are related by \\(Q_h\\to V_h, V_{h+1}\\to Q_h\\) \\[\\begin{align} V^\\pi_h(s) &amp;= \\mathop{\\mathbb{E}}\\limits_{a\\in \\pi(\\cdot\\mid s)}Q^\\pi_h(s, a) \\\\ Q^\\pi_h(s, a) &amp;= r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\in P(\\cdot\\mid s, a)}[V^\\pi_{h+1}(s&#39;)] \\end{align}\\] Substituting again yields the recursive relations \\(V_{h+1}\\to V_h, Q_{h+1}\\to Q_h\\). \\[\\begin{align} V^\\pi_h(s) &amp;= % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\in \\pi(\\cdot\\mid s) \\\\ s&#39;\\in P(\\cdot\\mid s, a)}}[r(s, a) + \\gamma\\, V^\\pi_{h+1}(s&#39;)] \\\\ Q^\\pi_h(s, a) &amp;= r(s, a) + \\gamma % \\mathop{\\mathbb{E}}\\limits_{\\substack{s&#39;\\in P(\\cdot\\mid s, a) \\\\ a&#39;\\in \\pi(\\cdot\\mid s&#39;)}} Q^\\pi_{h+1}(s&#39;, a&#39;) \\end{align}\\] These are known as the Bellman consistency relations. Definition 1.4 (Bellman consistency operators) We can rewrite the consistency relations in terms of consistency operators. Define the (value-) consistency operator \\(v\\mapsto \\mathcal J^\\pi_v\\) of type \\(\\mathcal J^\\pi_{(-)}: (\\mathcal S\\to \\mathbb R)\\to (\\mathcal S\\to \\mathbb R)\\). The action-value consistency operator \\(\\tilde{\\mathcal J}_{(-)}^\\pi\\) is defined similarly: \\[\\begin{align} \\mathcal J^\\pi_v(s) &amp;= % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\in \\pi(\\cdot\\mid s) \\\\ s&#39;\\in P(\\cdot\\mid s, a)}}[r(s, a) + \\gamma\\, v(s&#39;)] \\\\ \\tilde{\\mathcal J}_{q}^\\pi(s, a) &amp;= r(s, a) + \\gamma % \\mathop{\\mathbb{E}}\\limits_{\\substack{s&#39;\\in P(\\cdot\\mid s, a) \\\\ a&#39;\\in \\pi(\\cdot\\mid s&#39;)}} q(s&#39;, a&#39;) \\end{align}\\] It consumes a state-evaluation \\(v\\) and computes a state evaluation. Note that \\(\\mathcal J^\\pi_{(-)}\\) requires the environment dynamics \\(P(\\cdot\\mid s, a)\\) to be known. The consistency operators simplify the recursive definitions \\[ V^\\pi_h = \\mathcal J^\\pi_{V^\\pi_{h+1}},\\quad Q^\\pi_h = \\tilde {\\mathcal J}^\\pi_{Q^\\pi_{h+1}} \\] Optimality Definition 1.5 (optimal policy) A policy \\(\\pi^*\\) is optimal if \\[ \\mathop{\\mathbb{E}}\\limits_{s_0\\sim \\mu} V^{\\pi^*}_h(s_0) \\geq \\mathop{\\mathbb{E}}\\limits_{s_0\\sim \\mu} V^{\\pi}_0(s_0), \\quad \\forall \\pi \\in \\Pi \\] The optimal (action-) value function is defined analogously \\[\\begin{align} V^*_h(s) = \\max_\\pi V^\\pi_h(s), \\quad Q^*_h(s, a) = \\max_\\pi Q^\\pi_h(s, a) \\end{align}\\] Theorem 1.1 (optimality relations) \\(V^*\\) and \\(Q^*\\) again satisfy the recursive relations \\[\\begin{align} V^*_h(s) &amp;= \\max_a Q^*_h(s, a) \\\\ &amp;= \\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} V^*_{h+1}(s&#39;) \\right] \\\\ Q^*_h(s, a) &amp;= r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} V^*_{h+1}(s&#39;) \\\\ &amp;= r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} \\max_{a&#39;} Q^*_{h+1}(s&#39;, a&#39;) \\end{align}\\] Proof It suffices to prove \\(V^*_h(s) = \\max_a Q^*_h(s, a)\\) since the rest of the results follow from \\(V\\)-\\(Q\\) relations. Substitute the definition of \\(Q^*\\) to yield \\[\\begin{align} V^*_h(s) &amp;= \\max_\\pi V^\\pi_h(s) = \\max_{\\pi_h} \\max_{\\pi_{h+}} \\mathop{\\mathbb{E}}\\limits_{a\\sim \\pi_h} Q^{\\pi_{h+}}_{h+1}(s, a) \\\\ &amp;= \\max_{\\pi_h} \\mathop{\\mathbb{E}}\\limits_{a\\sim \\pi_h} \\left[ \\max_{\\pi_{h+}} Q^{\\pi_{h+}}_{h+1}(s, a) \\right] = \\max_a Q^*_h(s, a) \\end{align}\\] Definition 1.6 (optimality operators) Analogous to the consistency operators, define the \\(V\\)-optimality operator \\(\\mathcal J^*_{(-)}\\) and \\(Q\\)-optimality operator \\(\\tilde{\\mathcal J}^*_{(-)}\\) so that \\[\\begin{align} \\mathcal J^*_v(s) &amp;= \\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} v_{h+1}(s&#39;) \\right] \\\\ \\tilde {\\mathcal J}^*_q(s, a) &amp;= r(s, a) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} \\max_{a&#39;} q(s&#39;, a&#39;) \\end{align}\\] Remark. Note that the optimality equations have a very heavy dynamic programming flavor: solving \\(V^*\\) (or \\(Q\\)) at time \\(h+1\\) yields solutions for time \\(t\\). Definition 1.7 (greedy policy) Given action-value candidate \\(q:\\mathcal S\\times \\mathcal A\\to \\mathbb R\\), define the greedy policy \\(\\pi^q(a\\mid s) = 1_{a=\\operatorname*{arg\\,max}_{q}(s, a)}\\). Note that \\(q\\) doesn’t even have to satisfy the consistency equations. Theorem 1.2 ($Q^*$-greedy policy is optimal) Given optimal \\(Q^*\\), the greedy policy \\(\\pi^{Q^*}\\) is an optimal policy. This is nontrivial because greedy is a local behavior. In other words, \\[ V^{\\pi^{Q^*}} = V^* \\] Proof We prove this for the finite-horizon case. Optimality is trivial for terminal \\(t=H-1\\) Fix any \\(\\pi\\), inductively, \\[\\begin{align} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^\\pi} [r_h + \\dots + \\gamma^{\\dots}\\, r_{H+1}\\mid \\tau_h] &amp;= % \\mathop{\\mathbb{E}}\\limits_{\\substack{a_h\\sim \\pi_h(s_h) \\\\ s&#39;\\sim P(\\cdot\\mid s_h, a_h)}}\\left[ r_h(s_h, a_h) + \\gamma V^\\pi_{h+1}(s&#39;) \\right] \\\\ &amp;\\leq % \\mathop{\\mathbb{E}}\\limits_{\\substack{a_h\\sim \\pi_h(s_h) \\\\ s&#39;\\sim P(\\cdot\\mid s_h, a_h)}}\\left[ r_h(s_h, a_h) + \\gamma V^{\\pi^{Q^*}}_{h+1}(s&#39;) \\right] \\\\ &amp;= \\mathop{\\mathbb{E}}\\limits_{a_h\\sim \\pi_h(s_h)}Q^*_h(s, a) \\leq \\max_a Q^{\\pi^{Q^*}}_h(s, a) \\\\ &amp;= V^{\\pi^{Q^*}}_h(s) \\end{align}\\] In short: expand to rewrite using \\(V_{h+1}\\), apply inductive inequality, then regroup. Definition 1.8 (contraction) We equip the space of \\(v\\) and \\(q\\) functions with the \\(\\sup\\)-norm \\(\\|\\cdot\\|_\\infty\\) given by \\[ \\|v\\|_\\infty = \\sup_s |v(s)|, \\quad \\|q\\|_\\infty = \\sup_{s, a} |q(s, a)| \\] An operator \\(\\mathcal J\\) is a \\(\\gamma\\in (0, 1)\\) contraction if for any \\(u, v\\), we have \\[ \\|\\mathcal J(v) - \\mathcal J(u)\\| \\leq \\gamma \\|v-u\\| \\] Theorem 1.3 (contraction properties) For any policy \\(\\pi\\), the consistency operators \\(\\mathcal J^\\pi_{(-)}\\) and \\(\\mathcal{\\tilde J}^\\pi_{(-)}\\) are \\(\\gamma\\)-contractions. The optimality operators \\(\\mathcal J^*_{(-)}\\) and \\(\\mathcal{\\tilde J}^*_{(-)}\\) are also \\(\\gamma\\) contractions. Proof Recall the consistency operator definition 1.4; apply Jenson’s inequality and definition of the \\(\\sup\\)-norm: \\[\\begin{align} |\\tilde {\\mathcal J}^\\pi_q(s, a) - \\tilde {\\mathcal J}^\\pi_{q&#39;}(s, a)| &amp;= \\gamma \\left| % \\mathop{\\mathbb{E}}\\limits_{\\substack{s&#39;\\in P(\\cdot\\mid s, a) \\\\ a&#39;\\in \\pi(\\cdot\\mid s&#39;)}} q(s&#39;, a&#39;) - q&#39;(s&#39;, a&#39;) \\right| \\\\ &amp;\\leq \\gamma % \\mathop{\\mathbb{E}}\\limits_{\\substack{s&#39;\\in P(\\cdot\\mid s, a) \\\\ a&#39;\\in \\pi(\\cdot\\mid s&#39;)}} \\left| q(s&#39;, a&#39;) - q&#39;(s&#39;, a&#39;) \\right| \\leq \\gamma \\|q-q&#39;\\| \\end{align}\\] The argument for \\(V\\) follows similarly. For optimality operators, recall definition 1.6 and apply lemma 1.1 (fixing \\(s&#39;)\\): \\[\\begin{align} \\left| \\tilde {\\mathcal J}^*_q(s, a) - \\tilde {\\mathcal J}^*_{q&#39;}(s, a)\\right| &amp;= \\gamma \\left|\\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} \\left[ \\max_{a_0} q(s&#39;, a_0) - \\max_{a_1} q&#39;(s&#39;, a_1) \\right]\\right| \\\\ &amp;\\leq \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} \\left| \\max_{a_0} q(s&#39;, a_0) - \\max_{a_1} q&#39;(s&#39;, a_1) \\right| \\leq \\gamma \\|q-q&#39;\\| \\end{align}\\] Lemma 1.1 \\(|\\max_i a_i - \\max_j b_j| \\leq \\max_j |a_j - b_j|\\). Proof Let \\(\\max_j a_j = a_{j^*}\\), then \\[ \\max_i a_i - \\max_j b_j = a_{j^*} - \\max_j b_j \\leq a_{j^*} - b_{j^*} \\leq \\max_j |a_j - b_j| \\] Swap \\(a\\leftrightarrow b\\) to obtain the desired inequality. Policy evaluation Policy evaluation answers the following question: Given a policy \\(\\pi\\), what is \\(V^\\pi\\) and \\(Q^\\pi\\)? Remark (finite-horizon policy evaluation). In the finite horizon case, apply the consistency operators inductively to the trivial base case (terminal timestep). Algorithm: For \\(t=H-1\\dots 0\\) do: For each state \\(s\\), compute \\(\\mathcal J^\\pi_v(s)\\) (definition 1.4) by iterating over \\(a\\in \\mathcal A\\) and \\(s&#39;\\in \\mathcal S\\). The algorithmic complexity is \\(O(H\\cdot |\\mathcal A|\\cdot |\\mathcal S|^2)\\). The action-value function is defined similarly. Remark (infinite-horizon policy evaluation). For the infinite horizon case, We use the fixed-point relation \\(Q^\\pi = \\tilde{\\mathcal J}^\\pi_{Q^\\pi}\\) Since the consistency operator is a contraction, iterative contraction converges with rate \\(\\gamma\\). Algorithm: initialize \\(q_0(s, a)=0\\) and iteratively apply \\(q_{t+1} = \\tilde{\\mathcal J}^\\pi_{q_t}\\). Note that \\(\\|q_0 - Q^\\pi\\|\\leq R/(1-\\gamma)\\), where \\(R\\) is the uniform reward bound. To achieve \\(\\epsilon\\)-error, we need \\[\\begin{align} \\|q_t - Q^\\pi\\| &amp;\\leq \\gamma^{T(\\epsilon)} \\|q_0 - Q^\\pi\\| \\leq \\epsilon \\implies T(\\epsilon) \\geq \\dfrac 1 {1-\\gamma} \\log \\dfrac{R}{\\epsilon(1-\\gamma)} \\end{align}\\] Optimal solutions The ultimate goal of RL is to answer the following question: Given a MDP, how to compute the optimal policy? Definition 1.9 (finite-horizon (action-)value iteration) By theorem 1.2, computing \\(Q^*\\) suffices to yield the optimal policy. The value-iteration method uses the optimality recursion (definition 1.6) to compute \\(Q^*\\): Algorithm complexity is \\(O(H\\cdot |\\mathcal S|^2 \\cdot |\\mathcal A|)\\) when we compute \\(V^*\\). When computing \\(Q^*\\): Initialize \\(Q^*_{H-1}(s, a)=r(s, a)\\); this is the base case. For \\(t=H-2, \\dots, 0\\) do: For each \\(s, a\\), compute \\(Q^*_t(s, a) = \\tilde {\\mathcal J}^*_{Q^*_{t+1}}(s, a)\\) by iterating over \\(s&#39;\\sim P(\\cdot\\mid s, a)\\) and \\(a&#39;\\in \\mathcal A\\). Output the \\(Q^*\\)-greedy policy. Proposition 1.2 (infinite-horizon value iteration) For infinite MDPs, value iteration (VI) proceeds as follows: Initialize \\(v_0 = 0\\). Repeat until convergence: \\(v_{t+1} = \\mathcal J^*_{v_t}\\). Output the \\(v_T\\)-greedy policy \\(\\pi^{v_T}\\). Two remarks: \\(v_T\\) might not be a consistent Bellman value function. Moreover, the value function of the \\(v_T\\)-greedy policy is, in general, different from \\(v\\) We only have equality \\(V^{\\pi^{V^*}} = V^*\\) in the optimal case. We can derive an explicit bound on how well greedy policy “reconstructs” its value function: Theorem 1.4 (greedy policy value bound) Let \\(\\pi^*v\\) be the greedy policy w.r.t. \\(v:\\mathcal S\\to \\mathbb R\\) (note that \\(v\\) need not satisfy consistency relations), then \\[ \\|V^{\\pi^v} - V^* \\| \\leq \\dfrac{2\\gamma}{1-\\gamma} \\|v-V^*\\| \\] Proof Assuming deterministic policy and denote \\(\\hat \\pi = \\pi^v,\\hat V = V^{\\hat \\pi}\\) \\[\\begin{align} V^*(s) - \\hat V(s) &amp;= Q^*(s, \\pi^*(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s)) \\\\ &amp;= \\left[ Q^*(s, \\pi^*(s)) - Q^*(s, \\hat \\pi(s)) \\right] + \\left[ Q^*(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s)) \\right] \\end{align}\\] Let \\(q(s, a) = r(s, a) + \\gamma\\, \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} v(s&#39;)\\) so that \\(\\hat \\pi(a\\mid s) = 1_{a=\\operatorname*{arg\\,max}_{a} q(s, a)}\\), then note that by greedy construction of \\(\\hat \\pi\\), we have \\[ q(s, \\hat \\pi(s)) \\geq q(s, \\pi^*(s)) \\implies q(s, \\hat \\pi(s)) - q(s, \\pi^*(s)) \\geq 0 \\] Add this to the first square bracket to obtain \\[\\begin{align} Q^*(s, \\pi^*(s)) - Q^*(s, \\hat \\pi(s)) &amp;\\leq \\left[ Q^*(s, \\pi^*(s)) - q(s, \\pi^*(s)) \\right] + \\left[ q(s, \\hat \\pi(s)) - Q^*(s, \\hat \\pi(s)) \\right] \\\\ &amp;\\leq 2\\gamma \\|v-V^*\\| \\end{align}\\] The last inequality comes from expanding \\(q\\) and \\(Q^*\\). The second term is bounded similarly: \\[\\begin{align} Q^*(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s)) &amp;= \\gamma\\, \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, \\hat \\pi(s)} V^*(s&#39;) - v(s&#39;) \\\\ &amp;\\leq \\gamma \\|v-V^*\\| \\end{align}\\] Combining the results yields below; rearranging proves inequality, as claimed. \\[\\begin{align} \\|V^* - \\hat V\\| &amp;\\leq 2\\gamma \\|v-V^*\\| + \\gamma \\|V^* - \\hat V\\| \\end{align}\\] We can alternatively iterate the policy. Definition 1.10 (infinite-horizon policy iteration) Consider the following algorithm: Initialize a uniform policy \\(\\pi_0\\). Repeat until convergence: \\(\\pi_{t+1} = \\pi^{V^{\\pi_t}}\\). In human words: compute the value function of \\(\\pi_t\\) and act greedily w.r.t. the value function. The intuition behind this construction is that for the operator \\(\\mathcal G:\\pi \\mapsto \\pi^{V_\\pi}\\), the greedy-optimal operator \\(\\pi^{V^*}\\) is a fixed point. Theorem 1.5 (policy iteration converges optimally) Policy iteration converges to the optimal-greedy policy according to the rate \\[ \\| V^{\\mathcal G(\\pi)} - V^* \\| \\leq \\gamma \\|V^\\pi - V^*\\| \\] Proof The proof will be concluded if we show that \\(V^{\\mathcal G(\\pi)} \\geq \\mathcal J^*_{V^\\pi} \\geq V^\\pi\\), since together with the contraction property of the Bellman optimality operator (theorem 1.3), we have \\[ \\| V^* - V^{\\mathcal G(\\pi)}\\| \\leq \\| \\mathcal J^*_{V^\\pi} - V^* \\| \\leq \\gamma \\|V^\\pi - V^*\\| \\] First note that \\(\\mathcal J^*_{V^\\pi} \\geq V^\\pi\\), then \\[\\begin{align} V^{\\mathcal G(\\pi)}(s) - V^\\pi(s) &amp;\\geq V^{\\mathcal G(\\pi)}(s) - \\mathcal J^*_{V^\\pi}(s) \\\\ &amp;= \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, \\mathcal G_\\pi(s))} \\left[ V^{\\mathcal G(\\pi)}(s&#39;) - V^\\pi(s) \\right] \\end{align}\\] Apply this relationship recursively to yield \\(V^{\\mathcal G(\\pi)} \\geq V^\\pi\\). To see why the equation above holds, note that \\(\\mathcal G_\\pi(s)\\) is exactly the value which maximizes \\(Q^\\pi(s, a)\\), i.e.  \\[\\begin{align} \\mathcal J^*_{V^\\pi}(s) &amp;= \\max_a \\left[ r(s, a) + \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, a)} V^\\pi(s&#39;) \\right] \\\\ &amp;= r(s, \\mathcal G_\\pi(s)) + \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, \\mathcal G_\\pi(s))} V^\\pi(s) \\end{align}\\] Next, compute \\[\\begin{align} V^{\\mathcal G(\\pi)}(s) - \\mathcal J^*_{V^\\pi}(s) &amp;= \\gamma \\mathop{\\mathbb{E}}\\limits_{s&#39;\\sim P(\\cdot\\mid s, \\mathcal G_\\pi(s)}\\left[ V^{\\mathcal G(\\pi)}(s&#39;) - V^\\pi(s&#39;) \\right]_{\\geq 0} \\geq 0 \\end{align}\\] In short, the proof idea is: We need to show that \\(V^{\\mathcal G(\\pi)} \\geq \\mathcal J^*_{V^\\pi}\\) to apply the convergence theorem. We note that \\(V^{\\mathcal G(\\pi)} - J^*_{V^\\pi}\\) s an expectation over \\(V^{\\mathcal G(\\pi)} - V^\\pi\\), so it suffices to show that \\(V^{\\mathcal G(\\pi)} \\geq V^\\pi\\). To show the desired relation above, apply \\(V^\\pi \\leq \\mathcal J^*_{V^\\pi}\\) recursively. Remark. Note that the value function of a deterministic policy can be computed inclosed form for an infinite-horizon, finite-space MDP with deterministic rewards. Consider quantities of types \\[ P^\\pi\\in \\mathbb R^{|\\mathcal S|\\times |\\mathcal S|}, \\quad r, \\mu\\in \\mathbb R^{|\\mathcal S|} \\] Here \\(\\langle s&#39;|P^\\pi|s\\rangle= P(s&#39;\\mid s, \\pi(s))\\). In this notation, the Bellman consistency conditions can be solved in closed form as \\[ V^\\pi = r + \\gamma\\, P^\\pi V^\\pi \\implies V^\\pi = (I - \\gamma\\, P^\\pi)^{-1} r^\\pi \\] Remark (VI vs PI). Note that policy iteration (PI) cannot take more than \\(|\\mathcal A|^{|\\mathcal S|}\\) iterations since there are only so many deterministic policies. On the other hand, value iteration might take more since different value evaluations \\(\\neq\\) different greedy policies. Consider single-state MDP with \\(r(s, a)=1, \\gamma=.9\\) and \\(V_0=0\\). Then \\(V_1(s)=1\\) while \\(V^*(s)=10\\). "],["sampling-value-based-methods.html", "2 Sampling Value-based Methods Model-free policy evaluation Model free control", " 2 Sampling Value-based Methods This section explores RL solutions when the environment dynamics \\(P(\\cdot\\mid s, a)\\) and rewards are not known. This is also known as the model-free regime. We overcome the model-free constraint by using sampling (and consistent estimations) to replace expectations. TD learning is the direct extension of iteration-convergence methods. Policy evaluation + greedy improvement = solving the control problem (recall policy iteration). Policy evaluation methods: Monte-Carlo: high variance, no Markov conditions; converges to MSE-optimal estimates. Temporal Difference (TD): low variance, need Markov conditions; converges asymptotically (when run against static data) to DP solution on empirical distribution of data. TD in asymptotic batched setting = analytic solution of policy evaluation under empirical distribution. Difference between on-policy v. off-policy (definition 2.4), and online v. offline methods (definition 2.5. Action-value iteration is offline since we do not need to sample-approximate any \\(\\mathop{\\mathbb{E}}\\limits_{a\\sim \\pi(s)}\\). Policy-iteration is online since it involves policy evaluation. The three methods explored here are sampling approximations of the previous chapter’s methods: Monte-Carlo policy iteration = (policy evaluation using MC) + policy iteration: online &amp; on-policy. Temporal-Difference policy iteration (SARSA) = (policy evaluation using TD) + policy iteration: online &amp; on-policy. \\(Q\\)-learning 2.9 = action-value iteration. \\(Q\\)-learning is fundamentally a action-value iteration engine; online versions (@ref{prp:qVariants}) merely serve to obtain more relevant samples. Deep \\(Q\\)-learning = (\\(Q\\)-function approximation) + (sampling approximation) + (incremental online iteration to obtain relevant samples) + (action-value iteration). Target network &amp; experience replay buffers. Model-free policy evaluation We first consider the policy evaluation problem \\(\\pi \\mapsto Q^\\pi\\). We are more interested in \\(Q^\\pi\\) since \\(\\mathrm{argmax}\\) directly yields a greedy policy; in contrast, there’s no way to build a greedy policy from \\(V^\\pi\\) in the model-free regime. Definition 2.1 (evaluation criteria) We want our evaluation (estimation) protocol to be: Consistent: with enough data, estimate converges to the true value. \\[ \\lim_{n\\to \\infty} \\mathbb P(|\\hat \\theta_n - \\theta|&gt;\\epsilon_{\\forall}) = 0 \\] Computationally feasible: updating estimates is easy. Empirically estimate: MSE, bias, and variance. Recall that the return is defined as \\[ G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\] Definition 2.2 (Monte-Carlo Policy Evaluation) Given the following assumptions: All trajectories are finite. States and dynamics need not be Markov. Compute \\(V(s)\\) as follows: Sample episode \\(j = ((s_{jk}, a_{jk}, r_{jk}))_k\\). For the first occurence of each \\(s\\) in each episode, accumulate the empirical reward \\(G_{jt}\\mid s_{jt}=s\\). Return the average. Alternative to the first-visit method above, we can also compute the every-visit MC: e.g. for the first trajectory \\(s_0, s_1, s_0, \\dots\\), we’ll accumulate two samples \\(G_0, G_2\\) for \\(V(s_0)\\). We may also consider incremental MC by choosing a LR and set \\[ V^\\pi(s_{jt}) \\leftarrow V^\\pi(s_{jt}) + \\alpha[G_{jt} - V^\\pi(s_{jt})] \\] Remark. Monte-Carlo properties: First-visit MC is unbiased and consistent. Every-visit MC is biased but consistent, and usually smaller MSE (better sample efficiency). Incremental MC converges for \\(\\sum \\alpha_j = \\infty, \\quad \\sum \\alpha_j^2 &lt; \\infty\\). Limitations of MC: (1) high-variance, and (2) requires finite horizon: trajectories must end before updates can be computed. Recall the consistency relations \\[\\begin{align} V^\\pi_h(s) &amp;= % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\in \\pi(\\cdot\\mid s) \\\\ s&#39;\\in P(\\cdot\\mid s, a)}}[r(s, a) + \\gamma\\, V^\\pi_{h+1}(s&#39;)] \\\\ Q^\\pi_h(s, a) &amp;= r(s, a) + \\gamma % \\mathop{\\mathbb{E}}\\limits_{\\substack{s&#39;\\in P(\\cdot\\mid s, a) \\\\ a&#39;\\in \\pi(\\cdot\\mid s&#39;)}} Q^\\pi_{h+1}(s&#39;, a&#39;) \\end{align}\\] This suggests that we should nudge \\(V^\\pi(s) \\leftarrow \\mathcal J^\\pi_{V^\\pi}(s)\\); expectations can be replaced by sampling. Definition 2.3 (TD(0) algorithm) Given \\(\\pi\\), estimate \\(V^\\pi\\) as follows: Initialize \\(V^\\pi =0\\). Repeatedly sample tuple \\((s_t, a_t, r_t, s_{t+1})\\) and compute \\[ V^\\pi(s_t) \\leftarrow V^\\pi + \\alpha([r_t + \\gamma V^\\pi(s_{t+1})] - V^\\pi(s_t)) \\] One can also imagine “unrolling” the consistency relations twice (or more) before replacing expectations with sampling. This is the idea behind TD(\\(\\lambda\\)) learning. More unrolls nudge the algorithm closer to MC sampling, which has lower bias but higher variance. The same can be done for \\(Q^\\pi\\) by sampling \\((s_t, a_t, r_t, s_{t+1}\\); the expectation over \\(a&#39;\\) can be computed analytically since we have access to \\(\\pi\\). TD is a combination of Monte-Carlo (one-step) and dynamic programming methods! TD can be used for episodic or infinite-horizon settings. Markov condition is necessary since this underpins the validity of the Bellman consistency relations. Generally lower variance. Converges for \\(\\alpha_j\\) subject to the same conditions in MC. If \\(\\alpha=1\\), then TD in MDPs with stochastic choices of actions might oscillate forever (\\(\\alpha=1\\) results in really bad. Let us consider the batch learning scenario, where we have collected a finite number of samples and run the policy evaluation algorithms to convergence. Note that running TD twice on the same trajectory still generally updates the policy. The two methods will have different behaviors! Example 2.1 (Sutton &amp; Barto Ex 6.4) Consider two states \\(A, B\\) with \\(\\gamma=1\\). Suppose we have collected the following \\(8\\) episodes: \\(A, 0, B, 0\\). \\(B, 1\\) colleted \\(6\\) times. \\(B, 0\\). Then both TD and MC converge to \\(V(B)=0.75\\). However, \\(V_{\\mathrm{MC}}(A)=0\\) while \\(V_{\\mathrm{TD}}(A)=0.75\\). Theorem 2.1 (asymptotic batch MC and TD) In a batched setting, MC converges to values with minimum squared error, while TD(0) converges to DP policy for the MDP with MLE model estimates. To see why, recall that TD with convergent \\(\\alpha\\) is equivalent to iterated convergence using the consistency operator. Then running TD on batched data is equivalent to iterated policy evaluation on the empirical (MLE) distribution of the environment. In the example above, the DP structure “baked in” to the TD algorithm empowered more data-efficient results. Model free control Definition 2.4 (on (off)-policy algorithms) A learning algorithm is on policy if its update rule depends on samples using the current policy. Definition 2.5 (online (offline) algorithms) An online learning algorithm requires interaction with the environment during learning, while an offline algorithm operates with a static dataset. Online (offline) asks the question: does this interact with the environment during learning? On-policy asks the question: must update depend on data computed using the latest policy? In the previous section, we have seen how Monte-Carlo and TD can help evaluate policies; we have also seen how policy iteration 1.10 implies that policy evaluation + policy improvement (acting greedily w.r.t. evaluation) iterates to solve the control problem . Definition 2.6 (Monte-Carlo policy iteration) MCPI combines \\(Q\\)-evaluation using MC with policy iteration; it is online, on-policy. Algorithm: Initialize \\(Q=0\\) and \\(\\pi\\) acting \\(\\epsilon\\)-greedily w.r.t. \\(Q\\). Repeat until convergence: Sample trajectory \\(\\tau\\sim \\rho^\\pi\\) and extract \\(\\{(s_t, a_t, G_t)\\}\\) from trajectory. Update \\(Q(s_t, a_t) \\leftarrow \\bar \\alpha Q(s_t, a_t) + \\alpha G_t\\) (these two steps can be repeated before proceeding). Update \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(\\pi^{Q})\\). GLIE Monte-Carlo PI is guaranteed to converge to the optimal policy. Definition 2.7 (GLIE property) A learning strategy is Greedy in the Limit of Infinite Exploration (GLIE)if All state-action pairs are visited an infinite number of times. Behavior policy converges to greedy policy. A simple \\(\\epsilon\\)_scheduling GLIE policy sets \\(\\epsilon_k = 1/k\\). Definition 2.8 (temporal-difference policy iteration) (policy eval using TD) + (policy iteration) is also known as SARSA. It is an online, on-policy algorithm: Initialize \\(Q=0\\) and \\(\\epsilon\\)-greedy \\(\\pi\\). Repeat until convergence: Observe \\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\\). Update \\(Q(s_t, a_t) = \\bar \\alpha Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) \\right]\\). Note that alternatively, we only need \\(s_{t+1}\\) and replace \\(\\gamma Q(s_{t+1}, a_{t+1}) \\mapsto \\gamma \\mathop{\\mathbb{E}}\\limits_{a_{t+1}\\sim \\pi(s_{t+1})} Q(s_{t+1}, a_{t+1})\\). The previous two steps can be repeated multiple times before proceeding. Update \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(\\pi^{Q})\\). SARSA converges optimally assuming \\(\\pi_t\\) is GLIE (e.g. if \\(\\epsilon_t \\propto 1/t\\)) and if \\(\\alpha_t\\) is Robbins-Munro (\\(\\|\\alpha_t\\|_2 &lt; \\infty = \\|\\alpha_t\\|_1\\)). Definition 2.9 (Q-learning) \\(Q\\)-learning is the sampling variant of action-value iteration (definition 1.9). In its original form, it is offline, off-policy. Initialize \\(Q=0\\). Given a static list of trajectories, repeat until convergence: For each \\((s, a, r, s&#39;)\\): \\(Q(s, a) \\leftarrow \\bar \\alpha Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) \\right]\\). Repeat \\(Q\\)-greedy policy. Note that \\(Q\\)-learning in the asymptotic batch-setting limit is conceptually equivalent to solving MDP control on the empirical environment distribution. Proposition 2.1 (variants of Q-learning) Although \\(Q\\)-learning (sampling action-value iteration) is offline, to obtain more effective samples, a more prominent online version combines value iteration with policy iteration; this version is online, off-policy: Initialize \\(Q=0\\) and random policy \\(\\pi\\). Repeat until convergence: Generate trajectories using \\(\\pi\\). Repeat for a certain number of steps (or until convergence): For each \\((s, a, r, s&#39;)\\) in trajectory, update \\(Q(s, a)\\). Update \\(\\pi\\) to be an \\(\\epsilon\\)-greedy policy w.r.t. \\(Q\\). DQN (Deep Q Networks) build on the online version above with: Function approximation of \\(Q\\) using neural networks. To mitigate the non-i.i.d. data, use experience replay buffers. To mitigate nonstationary target in the Bellman optimality operator, update the \\(Q\\)-target slowly to improve stability. Intuition: while the Bellman optimality operator is a contraction, using functional approximation might break this property and lead to oscillation / divergence. "],["policy-methods.html", "3 Policy Methods Vanilla policy gradient Reducing variance Off-policy, conservative updates", " 3 Policy Methods In this section, we differentiably parameterize the policy and improve by gradient descent. While value methods solve control by computing the (action-) value functions, policy methods directly improve the policy. The Policy Gradient (PG) theorem 3.1 establishes the general form of score function \\(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\) multiplied by signal \\(R(\\tau)\\). Two problems with vanilla PG: high variance of signal and low sample efficiency from online learning Method to reduce variance: Baseline theorem 3.2 establishes that adjusting by a state and history-dependent function does not introduce bias. Baseline cannot depend on \\(a_t\\); can be \\(\\theta\\)-dependent; should be frozen during \\(\\theta\\)-gradient calculation. Remaining rewards, \\(Q(s, a)\\), and advantage \\(A=Q-V\\) are all valid signals (corollary 3.1). Generalized advantage estimation: interpolation between bootstrapping and sampling. Using off-policy data: intuitively, similar policies should be able to share data. Relative policy performance theorem 3.4: performance of \\(\\pi&#39;\\) is lower bounded by (performance of \\(\\pi\\)) + (a surrogate loss \\(\\mathcal L_\\pi(\\pi&#39;)\\)) + (square root of KL). Surrogate loss should be maximized (confusing entymology). Surrogate loss gradient degenerates to online PG gradient when \\(\\pi&#39;=\\pi\\). Theorem proof: performance difference lemma 3.3 + info-theory arguments (variational characterization of TV + Pinsker inequality). Implication: off-policy PG = optimizing surrogate-loss subject to small KL deviation Approximately enforce small-KL constraint by clipping: PPO-Clip 3.8. Full off-policy PG algorithm 3.7. Moving parts include: Estimating advantage: use MC or (how much) TD? Using another value-based approximation model is referred to as using a critic. Enforcing KL-constraints: clipping, adaptive-KL, or other methods. Balancing offline v. online: how many actor update steps prior to collecting a new batch of data? The core problem of RL is sampling efficiency; key obstacles include: Estimation efficacy: when estimating quantities such as policy-gradients, \\(V\\), or \\(Q\\), need to consider bias-variance tradeoff. On-policy data reuse: it takes nontrivial effort for data to be reusable for on-policy methods. Vanilla policy gradient Policy-based RL has better convergence properties, learns stochastic policies, and are often effective in high-dimensional or continuous spaces. On the other hand, convergence is local and evaluating a policy is often high-variance. Definition 3.1 (policy value) Fixing a MDP and parameterization \\(\\pi_\\theta\\), the policy value is written \\[ J(\\theta) = \\mathop{\\mathbb{E}}\\limits_{\\tau\\sim \\rho^{\\pi_\\theta}}[R(\\tau)] = \\mathop{\\mathbb{E}}\\limits_{\\tau\\sim \\rho^{\\pi_\\theta}}\\left[ \\sum_{t=0}^{H-1} r_t(\\tau)\\right] \\] The theorem below is foundational in policy methods. In particular, it does not require \\(R(\\tau)=\\sum \\gamma^t r_t\\) to be differentiable. Note that the \\(\\theta\\)-dependence is implicit through the trajectory measure \\(\\rho^{\\pi_\\theta}\\). Theorem 3.1 (Policy-Gradient) \\(\\nabla_\\theta J(\\theta) = \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[R(\\tau) \\sum_{t=0}^{H-1} \\nabla \\log \\pi_\\theta(a_t\\mid s_t)\\right]\\). Proof (log-EV trick) Note that \\(\\nabla_\\theta R(\\tau)=0\\) since the reward function \\(R\\) itself is independent of \\(\\theta\\), then \\[\\begin{align} \\nabla_\\theta J(\\theta) &amp;= \\nabla_\\theta \\int R(\\tau) \\rho^{\\pi_\\theta}(\\tau) \\, d\\tau \\\\ &amp;= \\int R(\\tau) \\nabla_\\theta \\rho^{\\pi_\\theta}(\\tau)\\, d\\tau \\\\ &amp;= \\int R(\\tau) \\rho^{\\pi_\\theta}(\\tau) \\dfrac{\\nabla_\\theta \\rho^{\\pi_\\theta}(\\tau)}{\\rho^{\\pi_\\theta}(\\tau)}\\, d\\tau \\\\ &amp;= \\mathop{\\mathbb{E}}\\limits_{\\tau}\\left[R(\\theta) \\nabla_\\theta \\log \\rho^{\\pi_\\theta}(\\tau)\\right] \\end{align}\\] where \\(\\dfrac 1 {g(\\theta)}\\nabla_\\theta g(\\theta) = \\nabla_\\theta \\log g(\\theta)\\). Proceeding to compute \\(\\nabla_\\theta \\log \\rho^{\\pi_\\theta}(\\tau)\\), the environment-dynamics log-probs are \\(\\theta\\)-independent, so \\[\\begin{align} \\nabla_\\theta \\log \\rho^{\\pi_\\theta}(\\tau) &amp;= \\nabla_\\theta \\sum_{t=0}^{H-1} \\log \\pi_\\theta(a_t\\mid s_t) + \\log P(s_{t+1}\\mid s_{t-1}, a_{t-1}) + \\text{ reward terms} \\\\ &amp;= \\sum_{t=0}^{H-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\end{align}\\] The \\(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\) is the parameter-space “direction” which will increase the policy’s popensity to choose \\(\\pi_\\theta(a_t\\mid s_t)\\), inversely weighted by the likelihood of the policy selecting that action. This normalization is necessary to offset the sampling weight in \\(\\mathbb E\\). The direction \\(\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\) is also known as the score function. Interpretation: the normalized policy directions \\(\\nabla \\log \\pi_\\theta(a_t\\mid s_t)\\) should be “reinforced” by the accumulated reward on the trajectory \\(R(\\theta)\\). The trajectory reward \\(R(\\tau)\\) is extremely high-variance. Intuition tells us that the reinforce signal for the action at time \\(t\\) should not be dependent upon past rewards \\(r_t, \\dots, r_{t-1}\\). This is in fact the case. We prove a more general result in the next subsection. Reducing variance Baseline To reduce variance, we prove that the gradient direction of \\(\\pi_\\theta(a_t\\mid s_t)\\) can be adjusted by a \\((s_0, \\dots, s_{t-1}, a_{t-1}, r_{t-1}, s_t)\\)-dependent baseline \\(B_t\\) without introducing bias, so that: \\(B_t=\\) accumulated rewards: \\(R_t-B_t=\\) remaining rewards is a valid signal. Taking expectation of remaining rewards: \\(Q^\\pi(s_t, a_t)\\) is a valid signal. \\(B_t\\) additionally includes \\(V(s_t)\\): the advantage function \\(A^\\pi=Q^\\pi-V^\\pi\\) is a valid signal. Theorem 3.2 (baseline trick) Fixing \\(t\\), given any deterministic function \\(B_t(\\sigma_t)\\) where \\(\\sigma_t(\\tau)=(s_0, a_0, r_0, \\dots, s_{t-1}, a_{t-1}, r_{t-1}, s_t)\\) is a slice of trajectory, we have \\[ \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ B_t(\\sigma_t(\\tau)) \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\right] = 0 \\] Note that inside the expectation, \\(\\sigma_t(\\tau)\\) will still be a random slice of the trajectory. Important proof: condition upon \\(s_0, \\dots, s_t\\) Let \\(\\mathcal F_t=\\sigma(s_0, a_0, r_0, \\dots, s_t)=\\mathcal F_t(\\sigma_t)\\) denote the history. The key property here is that the baseline \\(B_t(\\sigma_t)\\) is a constant w.r.t. this conditioning: \\[ \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ B_t(\\sigma_t) \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\mid \\mathcal F_t \\right] = B_t(\\sigma_t) \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\mid \\mathcal F_t \\right] \\] The remaining expectation vanishes, completing the proof: \\[\\begin{align} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\mid \\mathcal F_t\\right] &amp;= \\nabla_\\theta \\sum_{a_t, s_t} \\left[P(s_t\\mid s_{t-1}, a_{t-1}) \\pi_\\theta(a_t\\mid s_t)\\right] \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\\\ &amp;= \\nabla_\\theta \\sum_{a_t, s_t} P(s_t\\mid s_{t-1}, a_{t-1}) = 0 \\\\ \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[ \\nabla \\log \\pi_\\theta(a_t\\mid s_t)\\, B_t(\\tau) \\right] &amp;= \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[ \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[\\nabla \\log \\pi_\\theta(a_t\\mid s_t)\\mid \\mathcal F_t\\right]_{=0} \\, B_t(\\tau) \\right] = 0 \\end{align}\\] Note that the baseline function for the “reinforcement direction” (aka score function) of the conditional action \\(\\pi_\\theta(a_t\\mid s_t)\\) at time \\(t\\) can only depend on \\(s_t\\), not \\(a_t\\) or \\(r_t\\). The available variables \\(\\sigma_t(\\tau)\\) are chosen to satisfy the key equation \\[ \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ B_t(\\sigma_t) \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\mid \\mathcal F_t \\right] = B_t(\\sigma_t) \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t) \\mid \\mathcal F_t \\right] \\] Here the baseline \\(B_t(\\sigma_t)\\) must be a constant w.r.t. the conditioned variables while the score function remains r.v. Definition 3.2 (advantage function, remaining rewards) Given a policy \\(\\pi\\), the advantage function is defined as \\[ A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s, a) \\] The remaining reward of a trajectory at time \\(t\\) is defined as \\[ G_t(\\tau) = \\sum_{t&#39;=t}^{H-1} \\gamma^{t&#39;-t} r_{t&#39;}(\\tau) \\] Corollary 3.1 (valid PG signals with lower variance) Using remaining rewards, \\(Q\\)-function, or advantage function to reinforce action at time \\(t\\) all result in an unbiased estimator with lower variance: \\[\\begin{align} \\nabla_\\theta J(\\theta) &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[G_t(\\tau)\\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\right] \\\\ &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[Q^\\pi(s_t, a_t)\\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\right] \\\\ &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[A^\\pi(s_t, a_t)\\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\right] \\end{align}\\] Proof The first equation follows from the baseline theorem 3.2 by setting \\(B_t(\\sigma_t)\\) to be the accumulated rewards. We furnish another slick proof here: Assume for simplicity \\(\\gamma=1\\). Let \\(J_t(\\theta)=\\mathop{\\mathbb{E}}\\limits_{\\tau}[r_t(\\tau)]\\), then applying the REINFORCE theorem to the single reward term \\(J_t=\\mathop{\\mathbb{E}}\\limits_{\\tau}[r_t(\\tau)]\\) yields \\[\\begin{align} \\nabla_\\theta J_t(\\theta) &amp;= \\mathop{\\mathbb{E}}\\limits_{\\tau_{:t+1}}\\left[r_t(\\theta) \\nabla_\\theta \\log \\rho^{\\pi_\\theta}_{:t+1}(\\tau_{:t+1})\\right] \\end{align}\\] Note that here \\(r_t\\) is only dependent upon \\(\\tau_{:t+1}\\) (so includes up to \\(\\tau_t\\)). Since we’ve only sampled up to time \\(t\\), the \\(\\nabla_\\theta \\log \\rho^{\\pi_\\theta}_{:t+1}(\\tau_{:t+1})=\\sum_{t&#39;=0}^{t} \\nabla \\log \\pi_\\theta(a_{t&#39;}\\mid s_{t&#39;})\\). Rearranging sums and summing over all \\(t\\) completes the proof. To prove the validity of using \\(Q\\), invoke tower law w.r.t \\(\\mathcal F_t=\\sigma(s_0, \\dots, s_t, a_t)\\). Note that \\(\\mathbb E[G_t(\\tau)\\mid \\mathcal F_t] = \\mathbb E[r_{t}+\\dots\\mid s_t, a_t] = Q(s_t, a_t)\\): \\[\\begin{align} \\nabla_\\theta J(\\theta) &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[\\mathop{\\mathbb{E}}\\limits_{}[G_t(\\tau)\\mid \\mathcal F_t]\\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\right] \\\\ &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi_\\theta}}\\left[Q(s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\right] \\end{align}\\] Applying the baseline theorem to the result above proves the validity of the advantage function. Definition 3.3 (REINFORCE algorithm) Using corollary 3.1 above, consider the online, on-policy algorithm: Initialize stochastic policy \\(\\pi_\\theta\\). Repeat until convergence: Sample episode \\(\\{s_1, a_1, r_1, \\dots, s_H, a_H, r_H\\}\\sim \\rho^{\\pi_\\theta}\\): Compute \\(G_1, \\dots, G_H\\); note that these are treated as constants w.r.t. \\(\\theta\\). For \\(t=1\\dots H\\) update \\(\\theta\\mapsto \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(s_t, a_t)G_t\\). Return \\(\\pi_\\theta\\). Definition 3.4 (Monte-Carlo PG) Using the methods above, consider the following control algorithm: Initialize \\(\\pi_\\theta, B_\\phi\\). Repeat until convergence: Collect trajectories \\((\\tau_j)\\) on-policy. Compute \\(G_{jt}\\) for each policy and timestep. Fit the baseline by minimizing \\(\\sum_{j, t} |B_\\phi(s_{jt}) - G_{jt}|^2\\) (or use TD methods). Update policy \\(\\theta \\leftarrow \\theta + \\alpha\\, \\sum_{j, t} [G_{jt} - B_\\phi(s_{jt})]\\nabla_\\theta \\log \\pi_\\theta(a_{jt}\\mid s_{jt})\\). Remark. The algorithm above is equivalent to PG with (MC returns estimate + MC value baseline estimate). Other alternatives include: Estimate value baseline using TD. Bootstrap \\(G_{jt}\\) using TD instead of using MC. In these variations which introduce bootstrap, the policy model \\(\\pi_\\theta\\) is known as the actor, and the value model \\(B_\\phi\\) the critic. Also note that the baseline estimate can be \\(\\theta\\)-dependent, since the baseline trick does not require \\(B_t\\) to be \\(\\theta\\)-independent Generalized Advantage Estimation Another way of reducing bias is to introduce bootstrap the reinforcement signal. Consider the \\(N\\)-step advantage estimators: \\[\\begin{align} \\hat A_t^{(1)} &amp;= r_t + \\gamma V(s_{t+1}) - \\hat V(s_t) \\\\ \\hat A_t^{(2)} &amp;= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - \\hat V(s_t) \\\\ \\hat A_t^{(\\infty)} &amp;= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots - \\hat V(s_t) \\\\ \\end{align}\\] Note again that the reinforcement signal (advantage / Q-value / total reward) is frozen w.r.t. \\(\\nabla_\\theta\\). More steps imply (closer to MC) + (higher variance) + (lower bias). In either case, however, advantage is consistent so long as \\(\\hat V\\to V\\). Define the random variable \\[ \\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t) \\] It can be proven inductively that \\[\\begin{align} \\hat A^{(k)}_t = \\sum_{j=0}^{k-1} \\gamma^j r_{t+j} \\gamma^k \\hat V(s_{t+k}) - \\hat V(s_t) \\end{align}\\] Definition 3.5 (Generalized Advantage Estimator) GAE is an exponentially-weighted average of \\(k\\)-step advantage estimators which happen to have a very simple, tractable form. \\[\\begin{align} \\hat A_t^{\\mathrm{GAE}(\\gamma, \\lambda)} &amp;= (1-\\lambda) \\left(\\hat A_t^{(1)} + \\lambda \\hat A_t^{(2)} + \\lambda^2 \\hat A_t^{(3)} + \\dots\\right) = \\dots \\\\ &amp;= \\sum_{l=0}^\\infty (\\lambda \\gamma)^l \\delta^V_{t+l} \\end{align}\\] Note that \\(\\lambda=0\\) reduces to TD while \\(\\lambda=1\\) corresponds to MC. Off-policy, conservative updates The central improvements to PG all involve problems stemming from (1) high-variance of rewards, or (2) the online property: we only ever update policy based on trajectories collected using the current policy. The on-policy update rule results in low data efficiency. Parameterization: the norm in \\(\\theta\\)-space might be drastically different from that in \\(\\pi_\\theta\\) space. For example, when logits saturate in logistic / softmax parameterizations. Performance collapse: if \\(\\theta\\)-update results in bad policy, gradient samples and updates might be stuck. As a result, we want improvements to be close to original policy in KL; this mitigates performance collapse and approximately allows us to use off-policy data. One solution to (2) Natural gradints: coordinate-change by the (\\(\\pi_\\theta\\)-KL, \\(\\theta\\)-\\(L_2\\)) Hessian, which is the Fisher information; this coordinate map has trivial Jacobian generally. However, Fisher information matrix is prohibitive to compute generally. A more practical solution is to constrain the new policy to be close to the original in KL. Theorem 3.3 (performance difference lemma (PDL)) Given policies \\(\\pi, \\pi&#39;\\), define the induced state distribution \\[ d^{\\pi&#39;}(s) = (1-\\gamma)\\sum_{t=0}^\\infty \\gamma^t \\rho^{\\pi&#39;}(s_t=s) \\quad \\text{$H=\\infty$} \\] The policy performance difference can be written as \\[\\begin{align} J(\\pi&#39;) - J(\\pi) &amp;= \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi&#39;}} \\left[\\gamma^t A^\\pi(s_t, a_t)\\right] \\\\ &amp;= \\dfrac{1}{1-\\gamma} % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\sim \\pi&#39; \\\\ s\\sim d^{\\pi&#39;}}} \\left[A^\\pi(s, a)\\right] \\quad \\text{when $H=\\infty$} \\end{align}\\] Telescope proof: expand \\(A^\\pi\\) purely in terms of \\(V^\\pi\\) \\[\\begin{align} A^\\pi(s_t, a_t) &amp;= Q^\\pi(s_t, a_t) - V^\\pi(s_t) = r(s_t, a_t) + \\gamma \\mathop{\\mathbb{E}}\\limits_{s_{t+1}\\sim P(s_t, a_t)}[V^\\pi(s_{t+1})] - V^\\pi(s_t) \\end{align}\\] Sustituting this into the sum yields \\[\\begin{align} \\sum_{t=0}^{H-1} \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi&#39;}} \\left[\\gamma^t A^\\pi(s_t, a_t)\\right] &amp;= \\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi&#39;}} \\sum_{t=0}^{H-1} \\gamma^t \\left[ r(s_t, a_t) + \\gamma\\, V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right] \\\\ &amp;= \\left(\\mathop{\\mathbb{E}}\\limits_{\\tau \\sim \\rho^{\\pi&#39;}} \\sum_{t=0}^{H-1} \\gamma^t r_t \\right) + \\gamma^H V^\\pi(s_H)_{=0} - \\mathop{\\mathbb{E}}\\limits_{s_0}\\left[V^\\pi(s_0)\\right] \\\\ &amp;= J(\\pi&#39;) - J(\\pi) \\end{align}\\] For the finite-horizon case, PDL states that \\(J(\\pi&#39;) - J(\\pi)=\\) sum of \\(\\pi\\)’s advantage w.r.t the current state at each time step. The key equation is \\(A^\\pi(s_t, a_t) = r(s_t, a_t) + \\left[\\gamma \\mathop{\\mathbb{E}}\\limits_{\\text{env}}[V^\\pi(s_{t+1})] - V^\\pi(s_t)\\right]\\). Under \\(\\pi&#39;\\), the first term contributes \\(J(\\pi&#39;)\\), while the second term contributes \\(J(\\pi)\\) under telescoping. Definition 3.6 (surrogate loss) Denote the estimated performance of \\(\\pi&#39;\\) using \\(\\pi\\)-trajectories by \\(\\mathcal L_\\pi(\\pi&#39;)\\): \\[\\begin{align} \\mathcal L_\\pi(\\pi&#39;) &amp;= \\dfrac{1}{1-\\gamma} % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\sim \\pi \\\\ s\\sim d^{\\pi}}} \\left[\\dfrac{\\pi&#39;(a\\mid s)}{\\pi(a\\mid s)}A^\\pi(s, a)\\right] \\end{align}\\] The only difference from the analytic expression in theorem 3.3 is \\(d^{\\pi&#39;}\\mapsto d^\\pi\\). This is also known as the surrogate loss for off-policy PG. Theorem 3.4 (relative policy performance bounds) The performance difference estimate in theorem 3.3 is accurate up to expected KL of the policies (Achiam et al. 2017). \\[ \\left|\\left[J(\\pi&#39;) - J(\\pi)\\right] - \\mathcal L_\\pi(\\pi&#39;)\\right| \\leq C\\sqrt{\\mathop{\\mathbb{E}}\\limits_{s\\sim d^\\pi} D_{\\mathrm{KL}}\\left(\\pi&#39;(\\cdot\\mid s)\\| \\pi(\\cdot\\mid s)\\right)} \\] Rearranging and substituting \\(\\mathcal L_\\pi(\\pi&#39;)\\) yields the lower performance bound \\[\\begin{align} J(\\pi&#39;) &amp;\\geq J(\\pi) + \\dfrac{1}{1-\\gamma} % \\mathop{\\mathbb{E}}\\limits_{\\substack{a\\sim \\pi \\\\ s\\sim d^{\\pi}}} \\left[\\dfrac{\\pi&#39;(a\\mid s)}{\\pi(a\\mid s)}A^\\pi(s, a)\\right] - C\\sqrt{\\mathop{\\mathbb{E}}\\limits_{s\\sim d^\\pi} D_{\\mathrm{KL}}\\left(\\pi&#39;(\\cdot\\mid s)\\| \\pi(\\cdot\\mid s)\\right)} \\end{align}\\] Proof sketch Fixing \\(\\pi\\), define \\[ g(s) = \\dfrac{1}{1-\\gamma} \\mathop{\\mathbb{E}}\\limits_{a\\sim \\pi} \\left[\\dfrac{\\pi(a\\mid s)}{\\pi&#39;(a\\mid s)}A^\\pi(s, a)\\right] \\] So that \\(\\mathcal L_\\pi(\\pi&#39;) = \\mathop{\\mathbb{E}}\\limits_{s\\sim d^\\pi} g(s)\\) and \\(J(\\pi&#39;)-J(\\pi) = \\mathop{\\mathbb{E}}\\limits_{s\\sim d^{\\pi&#39;}}g(s)\\). Next, recall the variational characterization of total-variation distance (Polyanskiy and Wu 2025): let \\(\\mathcal F = \\{f:\\mathcal X\\to \\mathbb R, \\|f\\|_\\infty \\leq 1\\}\\), then \\[ \\mathrm{TV}(P, Q) = \\dfrac 1 2 \\sup_{f\\in \\mathcal F} \\left[\\mathbb{E}_P f(X) - \\mathbb{E}_Q f(X)\\right] \\] Using a suitably chosen constant so that \\(g/D\\in \\mathcal F\\), we have \\[ \\left|\\left[J(\\pi&#39;) - J(\\pi)\\right] - \\mathcal L_\\pi(\\pi&#39;)\\right| \\leq 2D \\, \\mathrm{TV}(\\pi, \\pi&#39;) \\] The bound follows from Pinsker’s inequality \\(D(P\\|Q)\\geq 2\\, \\mathrm{TV}(P, Q)^2\\). Corollary 3.2 (off-policy PG) Taking a step back, we have proven that a lower bound on \\(J(\\pi&#39;)\\) can be obtained from samples from \\(\\pi\\): \\[ \\text{Maximizing } J(\\pi&#39;_\\theta) \\text{ w.r.t. $\\theta$} \\impliedby \\text{ maximizing } \\mathcal L_\\pi(\\pi&#39;_\\theta) \\text{ w.r.t. $\\theta$} \\] when \\(D_{\\mathrm{KL}}\\left(\\pi&#39;(\\cdot\\mid s)\\| \\pi(\\cdot\\mid s)\\right)\\) is small. Note that the parameter-dependence of the old policy \\(\\pi\\) is irrelevant since \\(\\pi&#39;\\) uses new parameters. Further note that \\(\\nabla_\\theta \\mathcal L_\\pi(\\pi) = \\nabla_\\theta \\mathop{\\mathbb{E}}\\limits_{}[Q(s, a)] = \\nabla_\\theta J(\\pi_\\theta)\\), so we can just use the surrogate loss: \\[\\begin{align} \\nabla_\\theta \\mathop{\\mathbb{E}}\\limits_{\\pi_\\theta} \\left[ \\dfrac{\\pi_\\theta(a\\mid s)}{\\pi_\\theta(a\\mid s)_{\\text{frozen}}} A^{\\pi_\\theta}(s, a)_{\\text{frozen}} \\right] = \\mathop{\\mathbb{E}}\\limits_{\\pi_\\theta} \\left[ A^{\\pi_\\theta}(s, a)_{\\text{frozen}} \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\right] \\end{align}\\] Definition 3.7 (off-policy PG with surrogate loss) Consider the following algorithm: initialize \\(\\pi_\\phi\\) and repeat until convergence: Initialize \\(\\theta = \\phi\\). Collect \\(N\\) trajectories \\(\\tau_j\\); also collect \\(\\pi_\\phi(a_{jt}\\mid s_{jt})\\) for each move. Estimate advantages \\(A^{\\pi_\\phi}(s_{jt}\\mid s_{jt})\\approx Q^{\\pi_\\phi}(s_{jt}\\mid a_{jt}) - V^{\\pi_\\phi}(a_{jt})\\). Advantages are treated as constants in gradient computation. Repeat until convergence: Update \\(\\theta\\) with gradient \\(\\displaystyle \\dfrac{1}{(1-\\gamma)N} \\nabla_\\theta\\sum_{jt} [(1-\\gamma)\\gamma^t]\\dfrac{\\pi_{\\theta}(a_{jt}\\mid s_{jt})}{\\pi_\\phi(a_{jt}\\mid s_{jt})}A^{\\pi_\\phi}(s_{jt}\\mid s_{jt})\\). Optimization is performed subject to small KL. For finite-horizon tasks, replace \\(1/(1-\\gamma)\\mapsto H\\) and \\([(1-\\gamma)\\gamma^t]\\mapsto H\\) Update \\(\\phi \\leftarrow \\theta\\). Proceeding to solve the optimization problem subject to small \\(D_{\\mathrm{KL}}\\left(\\pi&#39;(\\cdot\\mid s)\\| \\pi(\\cdot\\mid s)\\right)\\). Recall that \\[ D(P\\|Q) = \\mathop{\\mathbb{E}}\\limits_{P}\\log \\dfrac{dP}{dQ} \\] We will be assured small KL if \\(\\pi&#39;(a|s)/\\pi(a|s)\\approx 1\\). One way to do so is to withold reinforcing feedback to increase (resp. decrease) \\(\\pi&#39;(a|s)\\) when \\(\\pi&#39;(a|s)/\\pi(a|s)\\) is greater (resp. less) than \\(\\pi(a|s)\\) by some amount. This results in PPO-clip. Definition 3.8 (PPO-Clip) Choose hyperparameter \\(\\epsilon&lt;1\\) (usually \\(0.1-0.3\\)); same as algorithm 3.7 except with \\[ r^\\theta_{jt} A^{\\pi_\\phi}(s_{jt}\\mid s_{jt}) \\mapsto \\min \\left[ r^\\theta_{jt} A_{jt}, \\mathrm{clip}(r^\\theta_{jt}, 1\\pm \\epsilon) A_{jt} \\right], \\quad r^\\theta_{jt} = \\dfrac{\\pi_{\\theta}(a_{jt}\\mid s_{jt})}{\\pi_\\phi(a_{jt}\\mid s_{jt})} \\] Bibliography Achiam, Joshua, David Held, Aviv Tamar, and Pieter Abbeel. 2017. “Constrained Policy Optimization.” In International Conference on Machine Learning, 22–31. PMLR. Polyanskiy, Yury, and Yihong Wu. 2025. Information Theory: From Coding to Learning. Cambridge university press. "],["bibliography.html", "Bibliography", " Bibliography Achiam, Joshua, David Held, Aviv Tamar, and Pieter Abbeel. 2017. “Constrained Policy Optimization.” In International Conference on Machine Learning, 22–31. PMLR. Polyanskiy, Yury, and Yihong Wu. 2025. Information Theory: From Coding to Learning. Cambridge university press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
