<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2025-07-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="mdp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#MDP"><i class="fa fa-check"></i>Preliminaries</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#vFunctions"><i class="fa fa-check"></i>Value functions</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#optimalityMDP"><i class="fa fa-check"></i>Optimality</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#policyEval"><i class="fa fa-check"></i>Policy evaluation</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#optimalSolutions"><i class="fa fa-check"></i>Optimal solutions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampMDP.html"><a href="sampMDP.html"><i class="fa fa-check"></i><b>2</b> Sample-based MDP solutions</a>
<ul>
<li class="chapter" data-level="" data-path="sampMDP.html"><a href="sampMDP.html#polEval"><i class="fa fa-check"></i>Model-free policy evaluation</a></li>
<li class="chapter" data-level="" data-path="sampMDP.html"><a href="sampMDP.html#modelFreeControl"><i class="fa fa-check"></i>Model free control</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="polMethods.html"><a href="polMethods.html"><i class="fa fa-check"></i><b>3</b> Policy Methods</a>
<ul>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pg"><i class="fa fa-check"></i>Vanilla policy gradient</a></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pgReduceVar"><i class="fa fa-check"></i>Reducing variance</a>
<ul>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pgBaseline"><i class="fa fa-check"></i>Baseline</a></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#advEstimate"><i class="fa fa-check"></i>Generalized Advantage Estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#ppo"><i class="fa fa-check"></i>Off-policy, conservative updates</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="iRL.html"><a href="iRL.html"><i class="fa fa-check"></i><b>4</b> Inverse RL</a>
<ul>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#imlEasy"><i class="fa fa-check"></i>Zeroth-order approaches</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#rewardShaping"><i class="fa fa-check"></i>Reward shaping</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#iRLClassical"><i class="fa fa-check"></i>Classical inverse RL</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#maxEntropyiRL"><i class="fa fa-check"></i>Max-entropy IRL</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="rlLLMs.html"><a href="rlLLMs.html"><i class="fa fa-check"></i><b>5</b> RL for LLMs</a>
<ul>
<li class="chapter" data-level="" data-path="rlLLMs.html"><a href="rlLLMs.html#rewardPreference"><i class="fa fa-check"></i>Preference-based reward modeling</a></li>
<li class="chapter" data-level="" data-path="rlLLMs.html"><a href="rlLLMs.html#dpo"><i class="fa fa-check"></i>Direct Preference Optimization</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="exploration.html"><a href="exploration.html"><i class="fa fa-check"></i><b>6</b> Exploration</a>
<ul>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#bandits"><i class="fa fa-check"></i>Multi-arm bandit</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#lrLB"><i class="fa fa-check"></i>Lai-Robins lower bound</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#ucb"><i class="fa fa-check"></i>Upper confidence bound method</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#bayesianBandit"><i class="fa fa-check"></i>Bayesian bandit</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#mdpExploration"><i class="fa fa-check"></i>Exploration in MDPs</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Reinforcement Learning</h1>
<p class="author"><em>Nicholas Lyu</em></p>
<p class="date"><em>2025-07-03</em></p>
</div>
<div id="preface" class="section level1 unnumbered hasAnchor">
<h1>Preface<a href="index.html#preface" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These notes accompany the summer 2025 self-learning for <em>Reinforcement Learning</em>.</p>
<ol style="list-style-type: decimal">
<li><a href="mdp.html#mdp">Markov Decision Processes</a> explores closed-form solutions to finite and infinite-horizon MDPs with <strong>known dynamics and rewards</strong>.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Recursive and optimality operators; contraction properties.</li>
<li>Control problem can be reduced to iterate (policy evaluation + greedy action), i.e.Â policy iteration.</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><a href="sampMDP.html#sampMDP">Sample-based MDP solutions</a> explores MDP solutions with <strong>unknown dynamics, known rewards</strong>.</li>
</ol>
<ul>
<li>Estimate procedures:
<ul>
<li>Monte-Carlo: high-variance no bias, no Markov assumptions, converges to MSE-optimal estimates.</li>
<li>Temporal difference: low variance high bias, need Markov conditions; converges to DP solution on empirical distribution</li>
</ul></li>
<li>Monte-Carlo policy iteration: MC + PI, online on-policy.</li>
<li>SARSA: TD policy iteration, online on-policy</li>
<li>Q-learning: action-value iteration; can use TD (canonical) or MC.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><a href="polMethods.html#polMethods">Policy Methods</a> explores an orthogonal approach by directly optimizing policies. Foundational policy gradient theorem <a href="polMethods.html#thm:PG">3.1</a> is improved in two areas:</li>
</ol>
<ul>
<li>Reducing variance: baseline theorem <a href="polMethods.html#thm:pgBaseline">3.2</a>.</li>
<li>Using off-policy data: relative policy difference theorem <a href="polMethods.html#def:surrogatePG">3.7</a>.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><a href="iRL.html#iRL">Inverse RL</a> explores MDP solutions with <strong>unknown rewards</strong>, replacing them with estimates from expert demonstration; orthogonal to dynamics estimation methods.</li>
</ol>
<ul>
<li>Main problem: identifiably problem of deducing rewards from expert policies; a partial answer is the reward shaping theorem <a href="iRL.html#thm:rewardShapingTheorem">4.1</a>.</li>
<li>Foundational result is <a href="iRL.html#maxEntropyiRL">max-entropy IRL</a> which has clean closed-form optimization.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><a href="rlLLMs.html#rlLLMs">RL for LLMs</a>: eliciting rewards from preferences; direct preference optimization.</li>
</ol>
<p>Cool theorems:</p>
<ul>
<li><a href="mdp.html#mdp">Markov Decision Processes</a>: contraction properties <a href="mdp.html#thm:contraction">1.3</a>, optimality of <span class="math inline">\(Q^*\)</span>-greedy policies <a href="mdp.html#thm:greedyPolicyOptimal">1.2</a>. Convergence bound of VI and PI.</li>
<li><a href="sampMDP.html#sampMDP">Sample-based MDP solutions</a>: NA, just definition.</li>
<li><a href="polMethods.html#polMethods">Policy methods</a>: PG theorem <a href="polMethods.html#thm:PG">3.1</a>, baseline theorem <a href="polMethods.html#thm:pgBaseline">3.2</a>, relative policy difference theorem <a href="polMethods.html#def:surrogatePG">3.7</a>.
<ul>
<li>Proof of relative policy performance theorem is a canonical example applying large-deviation theory and <span class="math inline">\(f\)</span>-divergences.</li>
</ul></li>
<li><a href="iRL.html#iRL">Inverse RL</a>: reward shaping theorem <a href="iRL.html#thm:rewardShapingTheorem">4.1</a>, max-entropy reduction theorem <a href="iRL.html#thm:maxEntReduction">4.2</a>.
<ul>
<li>Proof theme: Boltzmann methods, and interchange between single and nested optimizations.</li>
</ul></li>
<li><a href="rlLLMs.html#rlLLMs">RL for LLMs</a>: <a href="rlLLMs.html#dpo">DPO reduction</a>; variational characterization of KL and using analytic solutions to inner optimizations.</li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>

<a href="mdp.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
