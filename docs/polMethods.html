<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Policy Methods | Reinforcement Learning</title>
  <meta name="description" content="3 Policy Methods | Reinforcement Learning" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Policy Methods | Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Policy Methods | Reinforcement Learning" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2025-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sampMDP.html"/>
<link rel="next" href="iRL.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Processes</a>
<ul>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#MDP"><i class="fa fa-check"></i>Preliminaries</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#vFunctions"><i class="fa fa-check"></i>Value functions</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#optimalityMDP"><i class="fa fa-check"></i>Optimality</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#policyEval"><i class="fa fa-check"></i>Policy evaluation</a></li>
<li class="chapter" data-level="" data-path="mdp.html"><a href="mdp.html#optimalSolutions"><i class="fa fa-check"></i>Optimal solutions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sampMDP.html"><a href="sampMDP.html"><i class="fa fa-check"></i><b>2</b> Sample-based MDP solutions</a>
<ul>
<li class="chapter" data-level="" data-path="sampMDP.html"><a href="sampMDP.html#polEval"><i class="fa fa-check"></i>Model-free policy evaluation</a></li>
<li class="chapter" data-level="" data-path="sampMDP.html"><a href="sampMDP.html#modelFreeControl"><i class="fa fa-check"></i>Model free control</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="polMethods.html"><a href="polMethods.html"><i class="fa fa-check"></i><b>3</b> Policy Methods</a>
<ul>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pg"><i class="fa fa-check"></i>Vanilla policy gradient</a></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pgReduceVar"><i class="fa fa-check"></i>Reducing variance</a>
<ul>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#pgBaseline"><i class="fa fa-check"></i>Baseline</a></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#advEstimate"><i class="fa fa-check"></i>Generalized Advantage Estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="polMethods.html"><a href="polMethods.html#ppo"><i class="fa fa-check"></i>Off-policy, conservative updates</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="iRL.html"><a href="iRL.html"><i class="fa fa-check"></i><b>4</b> Inverse RL</a>
<ul>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#imlEasy"><i class="fa fa-check"></i>Zeroth-order approaches</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#rewardShaping"><i class="fa fa-check"></i>Reward shaping</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#iRLClassical"><i class="fa fa-check"></i>Classical inverse RL</a></li>
<li class="chapter" data-level="" data-path="iRL.html"><a href="iRL.html#maxEntropyiRL"><i class="fa fa-check"></i>Max-entropy IRL</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="rlLLMs.html"><a href="rlLLMs.html"><i class="fa fa-check"></i><b>5</b> RL for LLMs</a>
<ul>
<li class="chapter" data-level="" data-path="rlLLMs.html"><a href="rlLLMs.html#rewardPreference"><i class="fa fa-check"></i>Preference-based reward modeling</a></li>
<li class="chapter" data-level="" data-path="rlLLMs.html"><a href="rlLLMs.html#dpo"><i class="fa fa-check"></i>Direct Preference Optimization</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="exploration.html"><a href="exploration.html"><i class="fa fa-check"></i><b>6</b> Exploration</a>
<ul>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#bandits"><i class="fa fa-check"></i>Bandits</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#lrLB"><i class="fa fa-check"></i>Lai-Robins lower bound</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#ucb"><i class="fa fa-check"></i>UCB Bandit</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#bayesianBandit"><i class="fa fa-check"></i>Bayesian Bandits</a></li>
<li class="chapter" data-level="" data-path="exploration.html"><a href="exploration.html#mdpExploration"><i class="fa fa-check"></i>Exploration in MDPs</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="polMethods" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Policy Methods<a href="polMethods.html#polMethods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this section, we differentiably parameterize the policy and improve by gradient descent. While <strong>value methods</strong> solve control by computing the (action-) value functions, <strong>policy methods</strong> directly improve the policy.</p>
<ol style="list-style-type: decimal">
<li>The Policy Gradient (PG) theorem <a href="polMethods.html#thm:PG">3.1</a> establishes the general form of <em>score function</em> <span class="math inline">\(\nabla_\theta \log \pi_\theta(a_t\mid s_t)\)</span> multiplied by signal <span class="math inline">\(R(\tau)\)</span>.</li>
<li>Two problems with vanilla PG: <u>high variance of signal</u> and <u>low sample efficiency from online learning</u></li>
<li>Method to <a href="polMethods.html#pgReduceVar">reduce variance</a>:
<ul>
<li><strong>Baseline theorem <a href="polMethods.html#thm:pgBaseline">3.2</a></strong> establishes that <span style="color:blue">adjusting by a state and history-dependent function does not introduce bias. </span>
<ul>
<li>Baseline cannot depend on <span class="math inline">\(a_t\)</span>; can be <span class="math inline">\(\theta\)</span>-dependent; should be frozen during <span class="math inline">\(\theta\)</span>-gradient calculation.</li>
<li>Remaining rewards, <span class="math inline">\(Q(s, a)\)</span>, and advantage <span class="math inline">\(A=Q-V\)</span> are all valid signals (corollary <a href="polMethods.html#cor:lowvarPG">3.1</a>).</li>
</ul></li>
<li><a href="polMethods.html#advEstimate">Generalized advantage estimation</a>: interpolation between bootstrapping and sampling.</li>
</ul></li>
<li>Using <a href="polMethods.html#ppo">off-policy data</a>: intuitively, similar policies should be able to share data.
<ul>
<li><span style="color:blue"><strong>Relative policy performance theorem <a href="polMethods.html#thm:relPerfBound">3.4</a></strong>: performance of <span class="math inline">\(\pi&#39;\)</span> is lower bounded by (performance of <span class="math inline">\(\pi\)</span>) + (a surrogate loss <span class="math inline">\(\mathcal L_\pi(\pi&#39;)\)</span>) + (square root of KL). </span>
<ul>
<li>Surrogate loss should be <em>maximized</em> (confusing entymology).<br />
</li>
<li><u>Surrogate loss gradient degenerates to online PG gradient </u> when <span class="math inline">\(\pi&#39;=\pi\)</span>.</li>
<li>Theorem proof: performance difference lemma <a href="polMethods.html#thm:PDL">3.3</a> + info-theory arguments (variational characterization of TV + Pinsker inequality).</li>
</ul></li>
<li>Implication: <span style="color:blue"> off-policy PG = optimizing surrogate-loss subject to small KL deviation </span></li>
<li>Approximately enforce small-KL constraint by clipping: PPO-Clip <a href="polMethods.html#def:ppoClip">3.8</a>.</li>
</ul></li>
<li><strong>Full off-policy PG algorithm</strong> <a href="polMethods.html#def:surrogatePG">3.7</a>. Moving parts include:
<ul>
<li>Estimating advantage: use MC or (how much) TD? Using another value-based approximation model is referred to as using a <strong>critic</strong>.</li>
<li>Enforcing KL-constraints: clipping, adaptive-KL, or other methods.</li>
<li>Balancing offline v. online: how many actor update steps prior to collecting a new batch of data?</li>
</ul></li>
</ol>
<p>The core problem of RL is <strong>sampling efficiency</strong>; key obstacles include:</p>
<ul>
<li>Estimation efficacy: when estimating quantities such as policy-gradients, <span class="math inline">\(V\)</span>, or <span class="math inline">\(Q\)</span>, need to consider bias-variance tradeoff.</li>
<li>On-policy data reuse: it takes nontrivial effort for data to be reusable for on-policy methods.</li>
</ul>
<div id="pg" class="section level2 unnumbered hasAnchor">
<h2>Vanilla policy gradient<a href="polMethods.html#pg" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Policy-based RL has better convergence properties, learns stochastic policies, and are often effective in high-dimensional or continuous spaces.</li>
<li>On the other hand, convergence is local and evaluating a policy is often <strong>high-variance</strong>.</li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 3.1  (policy value) </strong></span>Fixing a MDP and parameterization <span class="math inline">\(\pi_\theta\)</span>, the policy value is written
<span class="math display">\[
    J(\theta) = \mathop{\mathbb{E}}\limits_{\tau\sim \rho^{\pi_\theta}}[R(\tau)]
    = \mathop{\mathbb{E}}\limits_{\tau\sim \rho^{\pi_\theta}}\left[
    \sum_{t=0}^{H-1} r_t(\tau)\right]
\]</span></p>
</div>
<p>The theorem below is foundational in policy methods. In particular, it does not require <span class="math inline">\(R(\tau)=\sum \gamma^t r_t\)</span> to be differentiable. Note that the <span class="math inline">\(\theta\)</span>-dependence is implicit through the trajectory measure <span class="math inline">\(\rho^{\pi_\theta}\)</span>.</p>
<div class="theorem">
<p><span id="thm:PG" class="theorem"><strong>Theorem 3.1  (Policy-Gradient) </strong></span><span class="math inline">\(\nabla_\theta J(\theta) = \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[R(\tau) \sum_{t=0}^{H-1} \nabla \log \pi_\theta(a_t\mid s_t)\right]\)</span>.</p>
</div>
<details>
<summary>
Proof (log-EV trick)
</summary>
Note that <span class="math inline">\(\nabla_\theta R(\tau)=0\)</span> since the reward function <span class="math inline">\(R\)</span> itself is independent of <span class="math inline">\(\theta\)</span>, then
<span class="math display">\[\begin{align}
    \nabla_\theta J(\theta)
    &amp;= \nabla_\theta \int R(\tau) \rho^{\pi_\theta}(\tau) \, d\tau \\
    &amp;= \int R(\tau) \nabla_\theta \rho^{\pi_\theta}(\tau)\, d\tau \\
    &amp;= \int R(\tau) \rho^{\pi_\theta}(\tau) \dfrac{\nabla_\theta \rho^{\pi_\theta}(\tau)}{\rho^{\pi_\theta}(\tau)}\, d\tau \\
    &amp;= \mathop{\mathbb{E}}\limits_{\tau}\left[R(\theta) \nabla_\theta \log \rho^{\pi_\theta}(\tau)\right]
\end{align}\]</span>
where <span class="math inline">\(\dfrac 1 {g(\theta)}\nabla_\theta g(\theta) = \nabla_\theta \log g(\theta)\)</span>. Proceeding to compute <span class="math inline">\(\nabla_\theta \log \rho^{\pi_\theta}(\tau)\)</span>, the environment-dynamics log-probs are <span class="math inline">\(\theta\)</span>-independent, so
<span class="math display">\[\begin{align}
    \nabla_\theta \log \rho^{\pi_\theta}(\tau)
    &amp;= \nabla_\theta \sum_{t=0}^{H-1} \log \pi_\theta(a_t\mid s_t) + \log P(s_{t+1}\mid s_{t-1}, a_{t-1}) + \text{ reward terms} \\
    &amp;= \sum_{t=0}^{H-1} \nabla_\theta \log \pi_\theta(a_t\mid s_t)
\end{align}\]</span>
</details>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(\nabla_\theta \log \pi_\theta(a_t\mid s_t)\)</span> is the parameter-space “direction” which will increase the policy’s popensity to choose <span class="math inline">\(\pi_\theta(a_t\mid s_t)\)</span>, <strong>inversely weighted</strong> by the likelihood of the policy selecting that action.
<ul>
<li>This normalization is necessary to offset the sampling weight in <span class="math inline">\(\mathbb E\)</span>.</li>
<li>The <em>direction</em> <span class="math inline">\(\nabla_\theta \log \pi_\theta(a_t\mid s_t)\)</span> is also known as the <strong>score function</strong>.</li>
</ul></li>
<li>Interpretation: <span style="color:blue">the normalized policy directions <span class="math inline">\(\nabla \log \pi_\theta(a_t\mid s_t)\)</span> should be “reinforced” by the accumulated reward on the trajectory <span class="math inline">\(R(\theta)\)</span>. </span></li>
<li>The trajectory reward <span class="math inline">\(R(\tau)\)</span> is extremely high-variance.</li>
</ol>
<p>Intuition tells us that the reinforce signal for the action at time <span class="math inline">\(t\)</span> <em>should not be dependent</em> upon past rewards <span class="math inline">\(r_t, \dots, r_{t-1}\)</span>. This is in fact the case. We prove a more general result in the next subsection.</p>
</div>
<div id="pgReduceVar" class="section level2 unnumbered hasAnchor">
<h2>Reducing variance<a href="polMethods.html#pgReduceVar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="pgBaseline" class="section level3 unnumbered hasAnchor">
<h3>Baseline<a href="polMethods.html#pgBaseline" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To reduce variance, we prove that the gradient direction of <span class="math inline">\(\pi_\theta(a_t\mid s_t)\)</span> can be adjusted by a <span class="math inline">\((s_0, \dots, s_{t-1}, a_{t-1}, r_{t-1}, s_t)\)</span>-dependent baseline <span class="math inline">\(B_t\)</span> without introducing bias, so that:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(B_t=\)</span> accumulated rewards: <span class="math inline">\(R_t-B_t=\)</span> remaining rewards is a valid signal.</li>
<li>Taking expectation of remaining rewards: <span class="math inline">\(Q^\pi(s_t, a_t)\)</span> is a valid signal.</li>
<li><span class="math inline">\(B_t\)</span> additionally includes <span class="math inline">\(V(s_t)\)</span>: the advantage function <span class="math inline">\(A^\pi=Q^\pi-V^\pi\)</span> is a valid signal.</li>
</ol>
<div class="theorem">
<p><span id="thm:pgBaseline" class="theorem"><strong>Theorem 3.2  (baseline trick) </strong></span>Fixing <span class="math inline">\(t\)</span>, given any deterministic function <span class="math inline">\(B_t(\sigma_t)\)</span> where <span class="math inline">\(\sigma_t(\tau)=(s_0, a_0, r_0, \dots, s_{t-1}, a_{t-1}, r_{t-1}, s_t)\)</span> is a slice of trajectory, we have
<span class="math display">\[
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[
        B_t(\sigma_t(\tau)) \nabla_\theta \log \pi_\theta(a_t\mid s_t)
    \right] = 0
\]</span>
Note that inside the expectation, <span class="math inline">\(\sigma_t(\tau)\)</span> will still be a random slice of the trajectory.</p>
</div>
<details>
<summary>
<em>Important proof</em>: condition upon <span class="math inline">\(s_0, \dots, s_t\)</span>
</summary>
<span style="color:blue"> Let <span class="math inline">\(\mathcal F_t=\sigma(s_0, a_0, r_0, \dots, s_t)=\mathcal F_t(\sigma_t)\)</span> denote the history. The <strong>key property</strong> here is that the baseline <span class="math inline">\(B_t(\sigma_t)\)</span> is a constant w.r.t. this conditioning:
<span class="math display">\[
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[
        B_t(\sigma_t) \nabla_\theta \log \pi_\theta(a_t\mid s_t)
    \mid \mathcal F_t \right]
    = B_t(\sigma_t) \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[
        \nabla_\theta \log \pi_\theta(a_t\mid s_t)
    \mid \mathcal F_t \right]
\]</span>
</span>
The remaining expectation vanishes, completing the proof:
<span class="math display">\[\begin{align}
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\mid \mathcal F_t\right]
    &amp;= \nabla_\theta \sum_{a_t, s_t} \left[P(s_t\mid s_{t-1}, a_{t-1}) \pi_\theta(a_t\mid s_t)\right] \nabla_\theta \log \pi_\theta(a_t\mid s_t) \\
    &amp;= \nabla_\theta \sum_{a_t, s_t} P(s_t\mid s_{t-1}, a_{t-1}) = 0 \\
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[
        \nabla \log \pi_\theta(a_t\mid s_t)\, B_t(\tau)
    \right]
    &amp;= \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[
        \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[\nabla \log \pi_\theta(a_t\mid s_t)\mid \mathcal F_t\right]_{=0} \, B_t(\tau)
    \right]
    = 0
\end{align}\]</span>
</details>
<p><span style="color:blue">
Note that the baseline function for the “reinforcement direction” (aka score function) of the conditional action <span class="math inline">\(\pi_\theta(a_t\mid s_t)\)</span> at time <span class="math inline">\(t\)</span> can only depend on <span class="math inline">\(s_t\)</span>, not <span class="math inline">\(a_t\)</span> or <span class="math inline">\(r_t\)</span>. The available variables <span class="math inline">\(\sigma_t(\tau)\)</span> are chosen to satisfy the key equation
<span class="math display">\[
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[
        B_t(\sigma_t) \nabla_\theta \log \pi_\theta(a_t\mid s_t)
    \mid \mathcal F_t \right]
    = B_t(\sigma_t) \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}} \left[
        \nabla_\theta \log \pi_\theta(a_t\mid s_t)
    \mid \mathcal F_t \right]
\]</span>
Here the baseline <span class="math inline">\(B_t(\sigma_t)\)</span> must be a constant w.r.t. the conditioned variables while the score function remains r.v.
</span></p>
<div class="definition">
<p><span id="def:advantage" class="definition"><strong>Definition 3.2  (advantage function, remaining rewards) </strong></span>Given a policy <span class="math inline">\(\pi\)</span>, the advantage function is defined as
<span class="math display">\[
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s, a)
\]</span>
The remaining reward of a trajectory at time <span class="math inline">\(t\)</span> is defined as
<span class="math display">\[
    G_t(\tau) = \sum_{t&#39;=t}^{H-1} \gamma^{t&#39;-t} r_{t&#39;}(\tau)
\]</span></p>
</div>
<div class="corollary">
<p><span id="cor:lowvarPG" class="corollary"><strong>Corollary 3.1  (valid PG signals with lower variance) </strong></span>Using <strong>remaining rewards</strong>, <strong><span class="math inline">\(Q\)</span>-function</strong>, or <strong>advantage function</strong> to reinforce action at time <span class="math inline">\(t\)</span> all result in an unbiased estimator with lower variance:
<span class="math display">\[\begin{align}
\nabla_\theta J(\theta)
&amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[G_t(\tau)\, \nabla_\theta \log \pi_\theta(a_t\mid s_t)\right] \\
&amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[Q^\pi(s_t, a_t)\,  \nabla_\theta \log \pi_\theta(a_t\mid s_t)\right] \\
&amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[A^\pi(s_t, a_t)\,  \nabla_\theta \log \pi_\theta(a_t\mid s_t)\right]
\end{align}\]</span></p>
</div>
<details>
<summary>
Proof
</summary>
<p>The first equation follows from the baseline theorem <a href="polMethods.html#thm:pgBaseline">3.2</a> by setting <span class="math inline">\(B_t(\sigma_t)\)</span> to be the accumulated rewards. We furnish another slick proof here:</p>
<p>Assume for simplicity <span class="math inline">\(\gamma=1\)</span>. Let <span class="math inline">\(J_t(\theta)=\mathop{\mathbb{E}}\limits_{\tau}[r_t(\tau)]\)</span>, then applying the REINFORCE theorem to the single reward term <span class="math inline">\(J_t=\mathop{\mathbb{E}}\limits_{\tau}[r_t(\tau)]\)</span> yields
<span class="math display">\[\begin{align}
    \nabla_\theta J_t(\theta)
    &amp;= \mathop{\mathbb{E}}\limits_{\tau_{:t+1}}\left[r_t(\theta) \nabla_\theta \log \rho^{\pi_\theta}_{:t+1}(\tau_{:t+1})\right]
\end{align}\]</span>
Note that here <span class="math inline">\(r_t\)</span> is only dependent upon <span class="math inline">\(\tau_{:t+1}\)</span> (so includes up to <span class="math inline">\(\tau_t\)</span>). Since we’ve only sampled up to time <span class="math inline">\(t\)</span>, the <span class="math inline">\(\nabla_\theta \log \rho^{\pi_\theta}_{:t+1}(\tau_{:t+1})=\sum_{t&#39;=0}^{t} \nabla \log \pi_\theta(a_{t&#39;}\mid s_{t&#39;})\)</span>. Rearranging sums and summing over all <span class="math inline">\(t\)</span> completes the proof.</p>
<p>To prove the validity of using <span class="math inline">\(Q\)</span>, invoke tower law w.r.t <span class="math inline">\(\mathcal F_t=\sigma(s_0, \dots, s_t, a_t)\)</span>. Note that <span class="math inline">\(\mathbb E[G_t(\tau)\mid \mathcal F_t] = \mathbb E[r_{t}+\dots\mid s_t, a_t] = Q(s_t, a_t)\)</span>:
<span class="math display">\[\begin{align}
    \nabla_\theta J(\theta)
    &amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[\mathop{\mathbb{E}}\limits_{}[G_t(\tau)\mid \mathcal F_t]\,
    \nabla_\theta \log \pi_\theta(a_t\mid s_t)\right] \\
    &amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi_\theta}}\left[Q(s_t, a_t)
    \nabla_\theta \log \pi_\theta(a_t\mid s_t)\right]
\end{align}\]</span></p>
Applying the baseline theorem to the result above proves the validity of the advantage function.
</details>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 3.3  (REINFORCE algorithm) </strong></span>Using corollary <a href="polMethods.html#cor:lowvarPG">3.1</a> above, consider the <strong>online, on-policy</strong> algorithm:</p>
<ol style="list-style-type: decimal">
<li>Initialize stochastic policy <span class="math inline">\(\pi_\theta\)</span>.</li>
<li>Repeat until convergence:
<ul>
<li>Sample episode <span class="math inline">\(\{s_1, a_1, r_1, \dots, s_H, a_H, r_H\}\sim \rho^{\pi_\theta}\)</span>:</li>
<li>Compute <span class="math inline">\(G_1, \dots, G_H\)</span>; note that these are treated as constants w.r.t. <span class="math inline">\(\theta\)</span>.</li>
<li>For <span class="math inline">\(t=1\dots H\)</span> update <span class="math inline">\(\theta\mapsto \theta + \alpha \nabla_\theta \log \pi_\theta(s_t, a_t)G_t\)</span>.</li>
</ul></li>
<li>Return <span class="math inline">\(\pi_\theta\)</span>.</li>
</ol>
</div>
<div class="definition">
<p><span id="def:pgMC" class="definition"><strong>Definition 3.4  (Monte-Carlo PG) </strong></span>Using the methods above, consider the following control algorithm:</p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\pi_\theta, B_\phi\)</span>.</li>
<li>Repeat until convergence:
<ul>
<li>Collect trajectories <span class="math inline">\((\tau_j)\)</span> on-policy.</li>
<li>Compute <span class="math inline">\(G_{jt}\)</span> for each policy and timestep.</li>
<li>Fit the baseline by minimizing <span class="math inline">\(\sum_{j, t} |B_\phi(s_{jt}) - G_{jt}|^2\)</span> (or use TD methods).</li>
<li>Update policy <span class="math inline">\(\theta \leftarrow \theta + \alpha\, \sum_{j, t} [G_{jt} - B_\phi(s_{jt})]\nabla_\theta \log \pi_\theta(a_{jt}\mid s_{jt})\)</span>.</li>
</ul></li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-13" class="remark"><em>Remark</em>. </span>The algorithm above is equivalent to PG with (MC returns estimate + MC value baseline estimate). Other alternatives include:</p>
<ul>
<li>Estimate value baseline using TD.</li>
<li>Bootstrap <span class="math inline">\(G_{jt}\)</span> using TD instead of using MC.</li>
</ul>
<p>In these variations which introduce bootstrap, the policy model <span class="math inline">\(\pi_\theta\)</span> is known as the <strong>actor</strong>, and the value model <span class="math inline">\(B_\phi\)</span> the <strong>critic</strong>.</p>
<p><span style="color:blue"> Also note that the baseline estimate <em>can</em> be <span class="math inline">\(\theta\)</span>-dependent, since the baseline trick does not require <span class="math inline">\(B_t\)</span> to be <span class="math inline">\(\theta\)</span>-independent</span></p>
</div>
</div>
<div id="advEstimate" class="section level3 unnumbered hasAnchor">
<h3>Generalized Advantage Estimation<a href="polMethods.html#advEstimate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another way of reducing bias is to introduce bootstrap the reinforcement signal. Consider the <span class="math inline">\(N\)</span>-step advantage estimators:</p>
<p><span class="math display">\[\begin{align}
    \hat A_t^{(1)}
    &amp;= r_t + \gamma V(s_{t+1}) - \hat V(s_t) \\
    \hat A_t^{(2)}
    &amp;= r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - \hat V(s_t) \\
    \hat A_t^{(\infty)}
    &amp;= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots - \hat V(s_t) \\
\end{align}\]</span></p>
<ul>
<li>Note again that the reinforcement signal (advantage / Q-value / total reward) is frozen w.r.t. <span class="math inline">\(\nabla_\theta\)</span>.</li>
</ul>
<p>More steps imply (closer to MC) + (higher variance) + (lower bias). In either case, however, advantage is consistent so long as <span class="math inline">\(\hat V\to V\)</span>. Define the random variable
<span class="math display">\[
    \delta_t^V = r_t + \gamma V(s_{t+1}) - V(s_t)
\]</span>
It can be proven inductively that
<span class="math display">\[\begin{align}
    \hat A^{(k)}_t
    = \sum_{j=0}^{k-1} \gamma^j r_{t+j} \gamma^k \hat V(s_{t+k}) - \hat V(s_t)
\end{align}\]</span></p>
<div class="definition">
<p><span id="def:gae" class="definition"><strong>Definition 3.5  (Generalized Advantage Estimator) </strong></span>GAE is an exponentially-weighted average of <span class="math inline">\(k\)</span>-step advantage estimators which happen to have a very simple, tractable form.
<span class="math display">\[\begin{align}
    \hat A_t^{\mathrm{GAE}(\gamma, \lambda)}
    &amp;= (1-\lambda) \left(\hat A_t^{(1)} + \lambda \hat A_t^{(2)} + \lambda^2 \hat A_t^{(3)} + \dots\right) = \dots \\
    &amp;= \sum_{l=0}^\infty (\lambda \gamma)^l \delta^V_{t+l}
\end{align}\]</span>
Note that <span class="math inline">\(\lambda=0\)</span> reduces to TD while <span class="math inline">\(\lambda=1\)</span> corresponds to MC.</p>
</div>
</div>
</div>
<div id="ppo" class="section level2 unnumbered hasAnchor">
<h2>Off-policy, conservative updates<a href="polMethods.html#ppo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The central improvements to PG all involve problems stemming from (1) high-variance of rewards, or (2) the online property: we only ever update policy based on trajectories collected <em>using the current policy</em>.</p>
<ol style="list-style-type: decimal">
<li>The on-policy update rule results in low data efficiency.</li>
<li>Parameterization: the norm in <span class="math inline">\(\theta\)</span>-space might be drastically different from that in <span class="math inline">\(\pi_\theta\)</span> space.
<ul>
<li>For example, when logits saturate in logistic / softmax parameterizations.</li>
</ul></li>
<li>Performance collapse: if <span class="math inline">\(\theta\)</span>-update results in bad policy, gradient samples and updates might be stuck.</li>
</ol>
<p>As a result, we want improvements to be close to original policy in KL; this mitigates performance collapse and approximately allows us to use off-policy data.</p>
<details>
<summary>
One solution to (2)
</summary>
<p>Natural gradints: coordinate-change by the (<span class="math inline">\(\pi_\theta\)</span>-KL, <span class="math inline">\(\theta\)</span>-<span class="math inline">\(L_2\)</span>) Hessian, which is the Fisher information; this coordinate map has trivial Jacobian generally.</p>
However, Fisher information matrix is prohibitive to compute generally. A more practical solution is to constrain the new policy to be close to the original in KL.
</details>
<div class="theorem">
<p><span id="thm:PDL" class="theorem"><strong>Theorem 3.3  (performance difference lemma (PDL)) </strong></span>Given policies <span class="math inline">\(\pi, \pi&#39;\)</span>, define the induced state distribution
<span class="math display">\[
    d^{\pi&#39;}(s)
    = (1-\gamma)\sum_{t=0}^\infty \gamma^t \rho^{\pi&#39;}(s_t=s) \quad \text{$H=\infty$}
\]</span>
The policy performance difference can be written as
<span class="math display">\[\begin{align}
    J(\pi&#39;) - J(\pi)
    &amp;= \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi&#39;}} \left[\gamma^t A^\pi(s_t, a_t)\right] \\
    &amp;= \dfrac{1}{1-\gamma} %
  \mathop{\mathbb{E}}\limits_{\substack{a\sim \pi&#39; \\ s\sim d^{\pi&#39;}}} \left[A^\pi(s, a)\right] \quad \text{when $H=\infty$}
\end{align}\]</span></p>
</div>
<details>
<summary>
Telescope proof: expand <span class="math inline">\(A^\pi\)</span> purely in terms of <span class="math inline">\(V^\pi\)</span>
</summary>
<span class="math display">\[\begin{align}
    A^\pi(s_t, a_t)
    &amp;= Q^\pi(s_t, a_t) - V^\pi(s_t)
    = r(s_t, a_t) + \gamma \mathop{\mathbb{E}}\limits_{s_{t+1}\sim P(s_t, a_t)}[V^\pi(s_{t+1})] - V^\pi(s_t)
\end{align}\]</span>
Sustituting this into the sum yields
<span class="math display">\[\begin{align}
    \sum_{t=0}^{H-1} \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi&#39;}} \left[\gamma^t A^\pi(s_t, a_t)\right] &amp;=
    \mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi&#39;}} \sum_{t=0}^{H-1} \gamma^t \left[
        r(s_t, a_t) + \gamma\, V^\pi(s_{t+1}) - V^\pi(s_t)
    \right] \\
    &amp;= \left(\mathop{\mathbb{E}}\limits_{\tau \sim \rho^{\pi&#39;}} \sum_{t=0}^{H-1} \gamma^t r_t \right) + \gamma^H V^\pi(s_H)_{=0} - \mathop{\mathbb{E}}\limits_{s_0}\left[V^\pi(s_0)\right] \\
    &amp;= J(\pi&#39;) - J(\pi)
\end{align}\]</span>
</details>
<ol style="list-style-type: decimal">
<li>For the finite-horizon case, PDL states that <span class="math inline">\(J(\pi&#39;) - J(\pi)=\)</span> sum of <span class="math inline">\(\pi\)</span>’s advantage w.r.t the current state at each time step.</li>
<li><span style="color:blue"> The key equation is <span class="math inline">\(A^\pi(s_t, a_t)  = r(s_t, a_t) + \left[\gamma \mathop{\mathbb{E}}\limits_{\text{env}}[V^\pi(s_{t+1})] - V^\pi(s_t)\right]\)</span>. Under <span class="math inline">\(\pi&#39;\)</span>, the first term contributes <span class="math inline">\(J(\pi&#39;)\)</span>, while the second term contributes <span class="math inline">\(J(\pi)\)</span> under telescoping.</li>
</ol>
<div class="definition">
<p><span id="def:pgSurrogate" class="definition"><strong>Definition 3.6  (surrogate loss) </strong></span>Denote the <span style="color:blue"> estimated performance of <span class="math inline">\(\pi&#39;\)</span> using <span class="math inline">\(\pi\)</span>-trajectories by <span class="math inline">\(\mathcal L_\pi(\pi&#39;)\)</span>:</span>
<span class="math display">\[\begin{align}
    \mathcal L_\pi(\pi&#39;)
    &amp;= \dfrac{1}{1-\gamma} %
  \mathop{\mathbb{E}}\limits_{\substack{a\sim \pi \\ s\sim d^{\pi}}} \left[\dfrac{\pi&#39;(a\mid s)}{\pi(a\mid s)}A^\pi(s, a)\right]
\end{align}\]</span>
The only difference from the analytic expression in theorem <a href="polMethods.html#thm:PDL">3.3</a> is <span class="math inline">\(d^{\pi&#39;}\mapsto d^\pi\)</span>. This is also known as the <strong>surrogate loss</strong> for off-policy PG.</p>
</div>
<div class="theorem">
<p><span id="thm:relPerfBound" class="theorem"><strong>Theorem 3.4  (relative policy performance bounds) </strong></span>The performance difference estimate in theorem <a href="polMethods.html#thm:PDL">3.3</a> is accurate up to expected KL of the policies <span class="citation">(<a href="#ref-achiam2017constrained">Achiam et al. 2017</a>)</span>.
<span class="math display">\[
    \left|\left[J(\pi&#39;) - J(\pi)\right] - \mathcal L_\pi(\pi&#39;)\right| \leq C\sqrt{\mathop{\mathbb{E}}\limits_{s\sim d^\pi} D_{\mathrm{KL}}\left(\pi&#39;(\cdot\mid s)\| \pi(\cdot\mid s)\right)}
\]</span>
Rearranging and substituting <span class="math inline">\(\mathcal L_\pi(\pi&#39;)\)</span> yields the lower performance bound
<span class="math display">\[\begin{align}
    J(\pi&#39;)
    &amp;\geq J(\pi) + \dfrac{1}{1-\gamma} %
  \mathop{\mathbb{E}}\limits_{\substack{a\sim \pi \\ s\sim d^{\pi}}} \left[\dfrac{\pi&#39;(a\mid s)}{\pi(a\mid s)}A^\pi(s, a)\right] - C\sqrt{\mathop{\mathbb{E}}\limits_{s\sim d^\pi} D_{\mathrm{KL}}\left(\pi&#39;(\cdot\mid s)\| \pi(\cdot\mid s)\right)}
\end{align}\]</span></p>
</div>
<details>
<summary>
Proof sketch
</summary>
Fixing <span class="math inline">\(\pi\)</span>, define
<span class="math display">\[
    g(s) = \dfrac{1}{1-\gamma} \mathop{\mathbb{E}}\limits_{a\sim \pi} \left[\dfrac{\pi(a\mid s)}{\pi&#39;(a\mid s)}A^\pi(s, a)\right]
\]</span>
So that <span class="math inline">\(\mathcal L_\pi(\pi&#39;) = \mathop{\mathbb{E}}\limits_{s\sim d^\pi} g(s)\)</span> and <span class="math inline">\(J(\pi&#39;)-J(\pi) = \mathop{\mathbb{E}}\limits_{s\sim d^{\pi&#39;}}g(s)\)</span>. Next, recall the <a href="https://nlyu1.github.io/classical-info-theory/f-divergence.html#tv-and-hellinger-hypothesis-testing">variational characterization of total-variation distance</a> <span class="citation">(<a href="#ref-polyanskiy2025information">Polyanskiy and Wu 2025</a>)</span>: let
<span class="math inline">\(\mathcal F = \{f:\mathcal X\to \mathbb R, \|f\|_\infty \leq 1\}\)</span>, then
<span class="math display">\[
    \mathrm{TV}(P, Q) = \dfrac 1 2 \sup_{f\in \mathcal F}
    \left[\mathbb{E}_P f(X) - \mathbb{E}_Q f(X)\right]
\]</span>
Using a suitably chosen constant so that <span class="math inline">\(g/D\in \mathcal F\)</span>, we have
<span class="math display">\[
    \left|\left[J(\pi&#39;) - J(\pi)\right] - \mathcal L_\pi(\pi&#39;)\right| \leq 2D \, \mathrm{TV}(\pi, \pi&#39;)
\]</span>
The bound follows from Pinsker’s inequality <span class="math inline">\(D(P\|Q)\geq 2\, \mathrm{TV}(P, Q)^2\)</span>.
</details>
<div class="corollary">
<p><span id="cor:unlabeled-div-14" class="corollary"><strong>Corollary 3.2  (off-policy PG) </strong></span>Taking a step back, we have proven that a lower bound on <span class="math inline">\(J(\pi&#39;)\)</span> can be obtained from samples from <span class="math inline">\(\pi\)</span>:
<span class="math display">\[
    \text{Maximizing } J(\pi&#39;_\theta) \text{  w.r.t. $\theta$}  \impliedby \text{ maximizing } \mathcal L_\pi(\pi&#39;_\theta) \text{    w.r.t. $\theta$}
\]</span>
<strong>when <span class="math inline">\(D_{\mathrm{KL}}\left(\pi&#39;(\cdot\mid s)\| \pi(\cdot\mid s)\right)\)</span> is small</strong>.</p>
</div>
<ol style="list-style-type: decimal">
<li>Note that the parameter-dependence of the old policy <span class="math inline">\(\pi\)</span> is irrelevant since <span class="math inline">\(\pi&#39;\)</span> uses new parameters.</li>
<li>Further note that <span class="math inline">\(\nabla_\theta \mathcal L_\pi(\pi) = \nabla_\theta \mathop{\mathbb{E}}\limits_{}[Q(s, a)] = \nabla_\theta J(\pi_\theta)\)</span>, so we can just use the surrogate loss:
<span class="math display">\[\begin{align}
\nabla_\theta \mathop{\mathbb{E}}\limits_{\pi_\theta} \left[
     \dfrac{\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)_{\text{frozen}}}
     A^{\pi_\theta}(s, a)_{\text{frozen}}
\right] = \mathop{\mathbb{E}}\limits_{\pi_\theta} \left[
     A^{\pi_\theta}(s, a)_{\text{frozen}} \nabla_\theta \log \pi_\theta(a\mid s)
\right]
\end{align}\]</span></li>
</ol>
<div class="definition">
<p><span id="def:surrogatePG" class="definition"><strong>Definition 3.7  (off-policy PG with surrogate loss) </strong></span>Consider the following algorithm:
initialize <span class="math inline">\(\pi_\phi\)</span> and repeat until convergence:</p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\theta = \phi\)</span>.</li>
<li>Collect <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\tau_j\)</span>; also collect <span class="math inline">\(\pi_\phi(a_{jt}\mid s_{jt})\)</span> for each move.</li>
<li>Estimate advantages <span class="math inline">\(A^{\pi_\phi}(s_{jt}\mid s_{jt})\approx Q^{\pi_\phi}(s_{jt}\mid a_{jt}) - V^{\pi_\phi}(a_{jt})\)</span>.
<ul>
<li>Advantages are treated as constants in gradient computation.</li>
</ul></li>
<li>Repeat until convergence:
<ul>
<li>Update <span class="math inline">\(\theta\)</span> with gradient <span class="math inline">\(\displaystyle \dfrac{1}{(1-\gamma)N} \nabla_\theta\sum_{jt} [(1-\gamma)\gamma^t]\dfrac{\pi_{\theta}(a_{jt}\mid s_{jt})}{\pi_\phi(a_{jt}\mid s_{jt})}A^{\pi_\phi}(s_{jt}\mid s_{jt})\)</span>.
<ul>
<li>Optimization is performed <strong>subject to small KL</strong>.</li>
<li>For finite-horizon tasks, replace <span class="math inline">\(1/(1-\gamma)\mapsto H\)</span> and <span class="math inline">\([(1-\gamma)\gamma^t]\mapsto H\)</span></li>
</ul></li>
</ul></li>
<li>Update <span class="math inline">\(\phi \leftarrow \theta\)</span>.</li>
</ol>
</div>
<p>Proceeding to solve the optimization problem subject to small <span class="math inline">\(D_{\mathrm{KL}}\left(\pi&#39;(\cdot\mid s)\| \pi(\cdot\mid s)\right)\)</span>. Recall that
<span class="math display">\[
    D(P\|Q) = \mathop{\mathbb{E}}\limits_{P}\log \dfrac{dP}{dQ}
\]</span>
We will be assured small KL if <span class="math inline">\(\pi&#39;(a|s)/\pi(a|s)\approx 1\)</span>. One way to do so is to withold reinforcing feedback to increase (resp. decrease) <span class="math inline">\(\pi&#39;(a|s)\)</span> when <span class="math inline">\(\pi&#39;(a|s)/\pi(a|s)\)</span> is greater (resp. less) than <span class="math inline">\(\pi(a|s)\)</span> by some amount. This results in PPO-clip.</p>
<div class="definition">
<p><span id="def:ppoClip" class="definition"><strong>Definition 3.8  (PPO-Clip) </strong></span>Choose hyperparameter <span class="math inline">\(\epsilon&lt;1\)</span> (usually <span class="math inline">\(0.1-0.3\)</span>); same as algorithm <a href="polMethods.html#def:surrogatePG">3.7</a> except with
<span class="math display">\[
    r^\theta_{jt} A^{\pi_\phi}(s_{jt}\mid s_{jt})
    \mapsto \min \left[
        r^\theta_{jt} A_{jt}, \mathrm{clip}(r^\theta_{jt}, 1\pm \epsilon) A_{jt}
    \right], \quad r^\theta_{jt} = \dfrac{\pi_{\theta}(a_{jt}\mid s_{jt})}{\pi_\phi(a_{jt}\mid s_{jt})}
\]</span></p>
</div>

</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-achiam2017constrained" class="csl-entry">
Achiam, Joshua, David Held, Aviv Tamar, and Pieter Abbeel. 2017. <span>“Constrained Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 22–31. PMLR.
</div>
<div id="ref-polyanskiy2025information" class="csl-entry">
Polyanskiy, Yury, and Yihong Wu. 2025. <em>Information Theory: From Coding to Learning</em>. Cambridge university press.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sampMDP.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="iRL.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
