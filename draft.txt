- Denote by $P$ the measure under $\{\mca R^a\}$ and $\tilde P$ the measure under the hypothetical setting. Note that since the only differing arm is $a$, by [KL-chain rule](https://nlyu1.github.io/classical-info-theory/kullback-leibler-divergence.html#prp:divergenceChainRule) we have 
\begin{align}
    D(P\|\tilde P) = \mbb E_P[N_a(t)]\,  D(\mca R^a \| \tilde {\mca R}^a) 
\end{align}
Note that crucially, the first factor is in $\mbb E_P$. 
- Fixing $t$, choose $E_t$ to be the event 
\[
    E_t = \left\{
        N_a(t) \leq (1-\epsilon) \df{\log t}{D(\mca R^a \|\tilde {\mca R}^a)} \equiv \alpha \log t 
    \right\} 
\] 
Substituting into the large-deviations lemma \@ref(lem:largeDeviations) yields 
\[ 
    \mbb E_P[N_a(t)]\,  D(\mca R^a \| \tilde {\mca R}^a) \geq P[E_t] \log \df 1 {Q[E_t]} 
\] 
Under setting $P$, the arm $a$ is suboptimal so $P[E_t] = 1 - o(1)$. On the other hand, arm $a$ is optimal under setting $Q$ so $\mbb E_{Q}[N_a(t)] = t - o(t)$. Applying Markov's inequality $P[X > a ] \leq \mbb E_P[X]/a$ to the complement, we obtain 
\begin{aligned}
    1 - Q[E_t] \leq \df{t - o(t)}{\alpha \log t} \implies Q[E_t] \geq 1 - \df{t-o(t)}{\alpha \log t}
\end{aligned}
Under the hypothetical setting $a$ is the best arm, so for good algorithm we have $Q(E_t)\to 0$.