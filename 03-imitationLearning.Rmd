# Imitation Learning

In this section, we explore RL when accurate reward models are hard to specify but are implicitly embedded in expert demonstrations. 

1. [Naively supervising](#imlEasy) based on the expert's behavior can be problematic due to state-distribution shifts. 
2. **Reward shaping theorem** \@ref(thm:rewardShapingTheorem): potential-based transformations $r(s, a, s')\mapsto r(s, a, s') + \gamma \Phi(s') - \Phi(s)$ is the only symmetry class <u>under all possible dynamics and baseline rewards</u>
    - Main idea: the transformation effects $V(s) \mapsto V(s) + \Phi(s)$ so the optimal policy remains invariant. 
    - Fixing $P$ and baseline $r$, there can be more symmetry.
3. Classical approach \@ref(def:classicalIRL) elucidates **adversarial** iteration between: 
    - Maximizing reward gap between expert reward and that of the current policy. 
    - Maximizing policy reward w.r.t. estimate. 

## Zeroth-order approaches {#imlEasy -}

:::{.definition #imlSetup name="problem setup"}
In the imitation setup, we have access to: 

- State and action spaces, transition model. 
- **No** reward model $R$. 
- Set of one or more teacher's demonstrations $(s_{jt}, a_{jt})$. 

Interesting tasks include: 

- **Behavior cloning**: how to reproduce the teacher's behavior?
- **Inverse RL**: how to recover $R$?
- **Apprenticeship learning via inverse RL**: use $R$ to generate a good policy. 
:::

:::{.definition #demoiml name="learning from demonstrations"}
Given demonstration trajectories $(s_{tj}, a_{tj})$ , train a policy with supervised learning. 
:::

One problem with behavior cloning: compounding errors. Supervised learning assumes $s_t\sim D_{\pi^*}$ i.i.d, while erroneous policies induce state distribution shift $s_t\sim D_{\pi_\theta}$ during test. 

A simple solution to this is called DAGGER, which iteratively asks the expert to provide feedback on the states visited by the policy. 

## Reward shaping {#rewardShaping -}

One immediate problem with learning the reward model is that the mapping $(R\to \pi^*)$ is not unique. One solution to this problem is provided in [@ng1999policy]. In full generality, we consider additive transformations $F(s, a, s')$ of the reward function. 

:::{.definition name="potential-based shaping function"}
A reward shaping function $F:S\times A\times R\to \R$ is a potential-based shaping function if there exists a real-valued function $\Phi:S\to \R$ suhch that $\forall s\in S-\{s_0\}$, 
\[ 
    F(s, a, s') = \gamma \Phi(s') - \Phi(s)
\] 
where $S-\{s_0\}=S$ if $\gamma<1$. 
:::

<div style="color:blue">
Two remarks in order about the following theorem: 

1. It includes scalar transformations $r\mapsto \alpha\, r$ as a special case. 
2. It uniquely identifies the symmetry group for $r\mapsto r+F$, <u>assuming that transition $P$ can be picked arbitrarily under picking the gauge</u>. Fixing the transition $P$ and baseline $r$ a priori, there might be a larger class of symmetries. 
</div> 

:::{.theorem #rewardShapingTheorem name="reward shaping theorem"}
The reward transformation $r\mapsto r+F$ preserves the optimal policy <span style="color:blue">for all transitions $P$ and baseline reward $r$</span> iff $F$ is a potential-based shaping function. In other words: 

- **Sufficiency**: if $F$ is potential-based, then every optimal policy under $r$ is an optimal policy in $r'=r+F$. 
- **Necessity**: if $F$ is not potential-based, then there exists transition models $P$ and reward function $R$ such that no optimal policy under $r'$ is optimal under $r$. 

Under this transformation, the value functions transform as 
\[ 
    Q(s, a)\mapsto Q(s, a) - \Phi(s), \quad V(s) \mapsto V(s) - \Phi(s) 
\] 
:::

<details>
<summary>Sufficiency: pick a transformation affecting $V^*\mapsto V^*+\pi$ which is independent of the policy</summary>
</details>

Let $M, M'$ denote MDPs under $r, r'=r+F$ respectively. Recall for $M^*$ the Bellman optimality equations: 
\[ 
    Q^*_M(s, a) 
    = \EV{s'\sim P(\cdot\mid s, a)}\left[
        r(s, a, s') + \gamma \max_{a'\sim A} Q^*M(s', a') 
    \right]
\] 
Subtract $\Phi(s)$ from both sides: 
\begin{align}
    Q^*_M(s, a) - \Phi(s) 
    = \EV{s'\sim P(\cdot\mid s, a)}\left[
        r(s, a, s') + \gamma\, \Phi(s') - \Phi(s) + \gamma \max_{a'\sim A} \left[
            Q^*M(s', a') - \Phi(s) 
        \right]
    \right]
\end{align}
But this is exactly the Bellman optimality equation for $M'$ with solution 
\[ 
    Q^*_{M'}(s, a) = Q^*_M(s, a) - \Phi(s)
\] 
Then any optimal policy for $M$ satisfying 
\[ 
    \pi^* = \argmax{\pi} V^*_M(s_0) =  \argmax{\pi} V^*_M(s_0) - \Phi(s_0) = \argmax{\pi} V^*_{M'}(s_0) - \Phi(s_0)
\] 
is also optimal for $M'$. 

## Inverse RL {#iRL -}

:::{.definition #linFeatureAssumption name="linear features"}
Assuming that we have a feature function $x:\mca S\times \mca A\to \R^n$ such that the reward is linear in features: 
\[ 
    r(s, a) = w^T x(s, a), \quad w\in \R^n\text{   and  } \|w_\infty\|_\leq 1 
\] 
Fixing features a priori, the goal of reward learning will be to identify the weight vector $w$ given a set of demonstrations. 
:::

:::{.proposition #featureMatchingLearning name="optimal-policy learning â‰ˆ feature matching"}
Given features $x$ satisfying assumptions \@ref(def:linFeatureAssumption) and policy $\pi$, define the **expected discounted feature** $\mu_\pi: \R^n$ by 
\[ 
    \mu_\pi =  \EV{\pi} \left[
        \sum_{t=0}^\infty \gamma^t x(s_t, a_t)\mid s_0 
    \right]
\] 
Assuming that $r=w^Tx$, then 
\[ 
    \|\mu_\pi - \mu_{\pi^*}\|_1\leq \epsilon \implies V^*(s_0) - V^\pi(s_0) \leq \epsilon
\] 
:::

Unrolling the linear reward function, $V^\pi$ can be rewritten as 
\[ 
    V^\pi(s_0) = \EV{\pi} \left[
        \sum_{t=0}^\infty \gamma^t w^T r(s_t, a_t)\mid s_0 
    \right]
    = w^T \mu_\pi 
\] 
Using Holder's inequality with $\|w\|_\infty \leq 1$, we obtain 
\[ 
    \|\mu_\pi - \mu_{\pi^*}\|_1\leq \epsilon \implies 
    |w^T\mu_\pi - w^T \mu_{\pi^*}| \leq \epsilon
\] 

:::{.definition #classicalIRL name="classical IRL algorithm"}
Assuming $r=w^Tx$ for features $x$ given a priori: 

1. Compute the optimal demonstration's discounted mean features $\mu_{\pi^*}$ from demonstration (proposition \@ref(prp:featureMatchingLearning)). 
2. Since the optimal policy satisfies $w^T \mu_{\pi^*} \geq w^T \mu_{\pi}$, initialize $\pi$ and repeat until convergence: 
    - Optimize $w\mapsto \argmax{\|w\|_\infty \leq 1 } w^T \mu_{\pi^*} - w^T \mu_\pi$. 
    - Iterate $\pi \mapsto \argmax{\pi} w^T \mu_\pi$. 
:::

## Max-entropy IRL {#maxEntropyiRL -}

One way of solving the identifiably problem is to use the principle of maximum entropy. In this section: 

1. We **assume knowledge** of the dynamics model $\pi$. 
2. Under this assumption, every policy $\pi_\theta$ induces a distribution $\rho^{\pi_\theta}$. 

The probability distribution in the admissible class of constraint-compatible distributions which best represents the current state knof knowledge is the one with largest entropy. 

<span style="color:blue"> In IRL setup, we wish to identify expert policy $\hat \pi^*$ from expert demonstration empirical distribution $\hat \rho = \frac 1 n \sum 1_{\tau_j}$. **Assuming a reward function**, identify a max-entropy distribution which is reward-equivalent to the empirical expert policy. 
</span>

:::{.definition #maxEntPrinciple name="principle of max entropy in IRL"}
Assuming access to the following components: 

1. Environment dynamics and a tentative reward model $r_\phi$. 
2. Expert empirical distribution $\hat \rho = \frac 1 n \sum 1_{\tau_j}$. 

The max-entropy trajectory distribution is specified by  
\[ 
    \rho^*_{r_\phi} = \max_\rho H(\rho) = -\sum_\tau \rho(\tau) \log \rho(\tau) 
\] 
subject to the following constraints: 

1. Normalization: $\sum_\tau \rho(\tau) = 1$. 
2. **Reward-equivalence** 
\[ 
    \EV{\tau \sim \rho}r_\phi(\tau) = \sum_\tau \rho(\tau) \, r_\phi(\tau) = \df 1 N \sum r_\phi(\tau_j) = \EV{\tau}\hat \rho(\tau)
\] 
:::

Note that the constraints above are equivalent to maximizing entropy subject to constant ($\rho$-independent) mean of $r_\phi(\tau)$ and being a valid probability distribution. The solution is, unsurprisingly, Boltzmann with free parameter $\lambda$: 
\[ 
    \rho^*_{r_\phi}(\tau) = \df{e^{\lambda r_\phi(\tau)}}{Z(r_\phi, \lambda)}, \quad Z(r_\phi, \lambda) = \sum_\tau e^{\lambda r_\phi(\tau)} 
\] 
<u> Note that the empirical average reward does not show up here</u>. W.l.o.g. we can absorb $\lambda$ into $r_\phi$ and set $\lambda=1$. Then 

\[ 
    \max_{r_\phi} \max_{\rho \text{ s.t. } \dots} H(\rho) 
    = \max_{r_\phi} H(\rho^*_{r_\phi}) 
\] 

:::{.definition}
Note that the const
:::